{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class = \"intro\">\n",
    "\n",
    "# Introduction to Machine Learning for Materials Science\n",
    "## Lab Activity\n",
    "\n",
    "<div class = \"cellContent\">\n",
    "    \n",
    "   \n",
    "This lab introduces students to an end-to-end example of applying a machine learning (ML) workflow to a materials science dataset to address a research problem. The lab aims at deepening the conceptual understanding of ML, and while procedural skills such as writing Python code are not the focus of this lab, students will gain experience with a number of standard open source packages by interacting with code snippets through the Jupyter Notebook format and describing what each essential command does.\n",
    "\n",
    "\n",
    "</div> <!-- closes content -->\n",
    "\n",
    "</div> <!-- closes intro -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "<div class = \"section toc\">\n",
    "\n",
    "\n",
    "## Table of Contents<a name=\"toc\"></a>\n",
    "\n",
    "<div class = \"cellContent\">\n",
    "    \n",
    "- ### [Lesson Format](#gs)\n",
    "- ### [Jupyter Notebooks Tips and Tricks](#jn)\n",
    "- ### [Section 0: Setup](#0)\n",
    "- ### [Section 1: Data Inspection](#1)\n",
    "    - ### [Importing Dataset](#lesson1.1)\n",
    "        - #### [Exercise 1.1](#e1.1)\n",
    "    - ### [Data Cleaning](#lesson1.2) \n",
    "        - #### [Exercise 1.2](#e1.2)\n",
    "        - #### [Exercise 1.3](#e1.3)\n",
    "    - ### [Evaluating Data Availability](#lesson1.3)\n",
    "        - #### [Exercise 1.4](#e1.4)\n",
    "        - #### [Exercise 1.5](#e1.5)\n",
    "        - #### [Exercise 1.6](#e1.6)\n",
    "- ### [Section 2: Feature Generation](#2)\n",
    "    - ### [MASTML Configuration](#lesson2.1)\n",
    "    - ### [Understanding Compositional Average Features](#lesson2.2)\n",
    "        - #### [Exercise 2.1](#e2.1)\n",
    "- ### [Section 3: Feature Engineering](#3)\n",
    "    - ### [Remove Constant Columns](#lesson3.1)\n",
    "        - #### [Exercise 3.1](#e3.1)\n",
    "    - ### [Remove Highly Correlated Columns](#lesson3.2)\n",
    "        - #### [Exercise 3.2](#e3.2)\n",
    "    - ### [Feature Normalization](#lesson3.3)\n",
    "- ### [Section 4: Setup for Model Evaluation](#4)\n",
    "    - ### [Establishing train/test split](#lesson4.1)\n",
    "    - ### [Evaluating train/test split](#lesson4.2)\n",
    "        - #### [Exercise 4.1](#e4.1)\n",
    "- ### [Section 5: Fitting and Evaluating a Default Model](#5)\n",
    "    - ### [Fitting the Decision Tree Model](#lesson5.1)\n",
    "    - ### [Evaluating Model Performance on Training Data](#lesson5.2)\n",
    "        - #### [Exercise 5.1](#e5.1)\n",
    "    - ### [Evaluating Model Performance on Test Data](#lesson5.3)\n",
    "        - #### [Exercise 5.2](#e5.2)\n",
    "- ### [Section 6: Hyperparameter Optimization](#6)\n",
    "    - ### [Establishing a cross-validation scheme](#lesson6.1)\n",
    "    - ### [Defining a parameter space](#lesson6.2)\n",
    "    - ### [Setting up a grid search](#lesson6.3)\n",
    "        - #### [Exercise 6.1](#e6.1)\n",
    "    - ### [Visualizing bias-variance tradeoff](#lesson6.4)\n",
    "        - #### [Exercise 6.2](#e6.2)\n",
    "    - ### [Default vs. optimized model: training and validation data performance](#lesson6.5)\n",
    "        - #### [Exercise 6.3](#e6.3)\n",
    "    - ### [Default vs. optimized model: test data performance](#lesson6.6)\n",
    "        - #### [Exercise 6.4](#e6.4)\n",
    "        - #### [Exercise 6.5](#e6.5)\n",
    "    - ### [Default vs. optimized model: visualizing the decision trees](#lesson6.7)\n",
    "        - #### [Exercise 6.6](#e6.7)\n",
    "        - #### [Exercise 6.7](#e6.7)\n",
    "- ### [Section 7: Make Predictions](#7)\n",
    "    - #### [Exercise 7.1](#e7.1)\n",
    "\n",
    "</div> <!--end content-->\n",
    "\n",
    "</div> <!--end section-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "<div class = \"button\">\n",
    "\n",
    "[\\[-----------------------------------Begin Lesson-----------------------------------\\]](#0) \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class = \"section\">\n",
    "\n",
    "# Setup<a name=\"0\"></a>\n",
    "---\n",
    "[Back to TOC](#toc)\n",
    "\n",
    "<div class = \"cellContent\">\n",
    "\n",
    "## Learning Outcomes\n",
    "1. Describe the purpose of essential Python packages used in this lab\n",
    "2. Recall that helper functions are available in this lab\n",
    "\n",
    "\n",
    "</div> <!-- closes content -->\n",
    "\n",
    "</div> <!-- closes section -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"lesson\">\n",
    "\n",
    "### Python Packages<a name = \"lesson0.1\"></a>\n",
    "\n",
    "\n",
    "<div class = \"cellContent\">\n",
    "\n",
    "There are a number of Python packages we'll use throughout the lab. Each of the import statements below imports or loads these packages in so that we can use them. A brief description is given for each one.\n",
    "\n",
    "</div> <!-- closes content -->\n",
    "\n",
    "</div> <!-- closes lesson -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                        # OS stands for Operating System and provides ways for python to interact with files or directories\n",
    "from collections import Counter  # Collections is a package for handling data\n",
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd              # Pandas is a data analysis library which we'll primarily use to handle our dataset\n",
    "import numpy as np               # Numpy is a package for scientific computing. We'll use it for some of it's math functions\n",
    "import pymatgen                  # Pymatgen is a library for materials analysis which we use to interpret our material compositions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mehr/anaconda3/lib/python3.11/site-packages/seaborn/_statistics.py:31: UserWarning: A NumPy version >=1.23.5 and <2.3.0 is required for this version of SciPy (detected version 1.23.1)\n",
      "  from scipy.stats import gaussian_kde\n"
     ]
    }
   ],
   "source": [
    "import matplotlib                # Matplotlib is the plotting package that we'll use throughout the lab\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns            # Seaborn is a Python data visualization library based on matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn                   # Scikit-learn is a machine learning package, providing the backbone for the work we'll perform\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_validate,GridSearchCV,ParameterGrid\n",
    "from sklearn.model_selection import KFold,RepeatedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgraphviz\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "import graphviz                  # graphviz is a package that helps visualize decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"lesson\">\n",
    "\n",
    "### Helper Functions<a name = \"lesson0.2\"></a>\n",
    "\n",
    "\n",
    "<div class = \"cellContent\">\n",
    "\n",
    "The final import we'll do is import some custom functions that mainly put together the plots and some specific analysis that we'll use throughout the lab. If you're interested in learning what exactly these look like you can open up the helper_functions.py text file that is in the same folder as this notebook.\n",
    "\n",
    "</div> <!-- closes content -->\n",
    "\n",
    "</div> <!-- closes lesson -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'helper_functions'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhelper_functions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'helper_functions'"
     ]
    }
   ],
   "source": [
    "from helper_functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of steps we'll take which would normally have a random state. In order to have consistent results we'll fix them all by setting a random seed for all those processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 2345312"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"section\">\n",
    "\n",
    "# Materials Background<a name=\"0.5\"></a>\n",
    "___\n",
    "\n",
    "[Back to TOC](#toc)\n",
    "\n",
    "<div class = \"cellContent\">\n",
    "    \n",
    "## Band Gap Dataset\n",
    "Throughout the lab we'll be working with a digitized version of a band gap dataset compiled by two scientists, W.H. Strehlow and E.L. Cook. They compiled this dataset of elementary and binary compound semiconductors in 1973 by searching through 723 individual references. Along with simply compiling reported measurements they also took the time to identify more and less reliable data points by taking \"into consideration the material, the method of measurement, the reported sample purity, and the experimental conditions.\" They also comment that experimental measures are often greater than 5% in error and seldom less than 1%. \n",
    "\n",
    "https://citrination.com/datasets/1160/show_files\n",
    "\n",
    "\n",
    "## Uses In Materials Science\n",
    "Having access to a dataset such as this is a great place to start for machine learning. In this case band gap data has been generated over many years, and often times the first step in any machine learning problem is simply acquiring the data. Luckily Strehlow and Cook have done the largest part of the work for us already! So now let's think about how we might want to use the data that we have. We'll focus on two potential applications for our machine learning models, which will give us something to call back to when we want to decide how well the models are performing\n",
    "\n",
    "### Predicting Materials for Single-Junction Solar Cells\n",
    "Solar power is growing as a renewable energy source and there are two main factors that could improve it's viability when competing with other sources of energy. First is the cost of manufacturing materials, and the second is the efficiency of the Solar Cells themselves. If cost could be lowered via the discovery of new materials that are cheaper, and the efficiency of cells can be improved by finding materials with ideal band gaps, then solar energy could grow faster! So lets set our first goal as being a model that can predict potential solar materials based on their band gap. For simplicity we'll limit ourselves to single-junction cells, which allows us to more clearly define an operational range of band gaps. Following the suggestions from the links below (which cover things in much greater detail), in order to obtain reasonable efficiency in a single-junction solar cell we would need a material with a band gap in the range of 1.0 eV to 1.7 eV. In order to make predictions in this range then, we would need a model with an accuracy of +- 0.35 eV at the most to give us a shot and having predictions actually be within that range. \n",
    "\n",
    "For a more thorough background see the following:\n",
    "http://solarcellcentral.com/junction_page.html\n",
    "http://solarcellcentral.com/limits_page.html\n",
    "\n",
    "\n",
    "### Predicting Wide Band Gap Semi-conductors\n",
    "There are also a range of semi-conductor devices that are relatively less well known which fall under the category of Wide Band Gap semi-conductors (WBG). A few example devices would be in industrial motors where they can improve efficiency, in LED lighting and lasers, and in general electronics. Several properties of WBGs that give these device improvements are their higher voltage, temperature, and frequency operation ranges. For a more thorough discussion see the linked review! So again if we can build a model to predict WBG materials, we can potentially accelerate development of these next generation devices. Our criteria for predicting WBG materials will be having a bandgap between 2 eV and 4 eV. whereas more traditional semi-conductors have band gaps below 2 eV. That means for predicting WBG materials we need a model with an accuracy of +- 1 eV!\n",
    "\n",
    "Reference: https://www1.eere.energy.gov/manufacturing/rd/pdfs/wide_bandgap_semiconductors_factsheet.pdf\n",
    "\n",
    "</div> <!-- closes content -->\n",
    "\n",
    "</div> <!-- closes section -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"section\">\n",
    "\n",
    "# Data Cleaning and Inspection<a name=\"1\"></a>\n",
    "___\n",
    "\n",
    "[Back to TOC](#toc)\n",
    "\n",
    "<div class = \"cellContent\">\n",
    "    \n",
    "## Overview\n",
    "Before getting started, let's first get to know the dataset we'll be working with a little. We'll be taking a look at what information is contained in each column, and how we can use that information to set ourselves up for success later when we start building our machine learning models. Remember that machine learning models are looking for patterns in the data! So any cleaning or pre-processing that we're doing here is with the goal of making those patterns easier to learn!\n",
    "\n",
    "## Learning Outcomes\n",
    "1. Recall common issues with materials datasets that require data cleaning\n",
    "2. Recognize effect of data cleaning steps on the dataset\n",
    "3. Evaluate the dataset using criteria of data cleaning\n",
    "4. Evaluate feasibility of different ML applications given the available data\n",
    "\n",
    "\n",
    "\n",
    "</div> <!-- closes content -->\n",
    "\n",
    "</div> <!-- closes section -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content/Exercises/Lessons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"lesson\">\n",
    "\n",
    "### Importing the Dataset<a name = \"lesson1.1\"></a>\n",
    "\n",
    "\n",
    "<div class = \"cellContent\">\n",
    "\n",
    "Through out this lab, you will see many objects in the code named as `_df`, which stands for <u>dataframe</u>, the primary data structure from the `pandas` package. It is similar to a spreadsheet or a table.\n",
    "\n",
    "We won't discuss dataframe operations in detail in this lab. However, external resources are available for you to familiarize yourself with dataframes such as [this introduction](https://towardsdatascience.com/pandas-dataframe-a-lightweight-intro-680e3a212b96) and [the pandas documentation](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.html).\n",
    "\n",
    "</div> <!-- closes content -->\n",
    "\n",
    "</div> <!-- closes lesson -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the band gap data from our dataset\n",
    "mastml_df = pd.read_csv(\"bandgap_data_v2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>chemicalFormula Clean</th>\n",
       "      <th>Band gap values Clean</th>\n",
       "      <th>Band gap units</th>\n",
       "      <th>Band gap method</th>\n",
       "      <th>Reliability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Li1F1</td>\n",
       "      <td>13.60</td>\n",
       "      <td>eV</td>\n",
       "      <td>Reflection</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Li1F1</td>\n",
       "      <td>12.61</td>\n",
       "      <td>eV</td>\n",
       "      <td>Reflection</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Li1F1</td>\n",
       "      <td>12.60</td>\n",
       "      <td>eV</td>\n",
       "      <td>Estimated</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Li1F1</td>\n",
       "      <td>12.10</td>\n",
       "      <td>eV</td>\n",
       "      <td>Absorption</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Li1F1</td>\n",
       "      <td>12.00</td>\n",
       "      <td>eV</td>\n",
       "      <td>Absorption</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1442</th>\n",
       "      <td>1454</td>\n",
       "      <td>Th1O2</td>\n",
       "      <td>3.30</td>\n",
       "      <td>eV</td>\n",
       "      <td>Reflection</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>1455</td>\n",
       "      <td>UO</td>\n",
       "      <td>1.50</td>\n",
       "      <td>eV</td>\n",
       "      <td>Thermal activation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>1456</td>\n",
       "      <td>U1O2</td>\n",
       "      <td>2.18</td>\n",
       "      <td>eV</td>\n",
       "      <td>Absorption</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1445</th>\n",
       "      <td>1457</td>\n",
       "      <td>UO</td>\n",
       "      <td>0.60</td>\n",
       "      <td>eV</td>\n",
       "      <td>Thermal activation</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446</th>\n",
       "      <td>1458</td>\n",
       "      <td>U1O2</td>\n",
       "      <td>1.30</td>\n",
       "      <td>eV</td>\n",
       "      <td>Thermal activation</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1447 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index chemicalFormula Clean  Band gap values Clean Band gap units  \\\n",
       "0         0                 Li1F1                  13.60             eV   \n",
       "1         1                 Li1F1                  12.61             eV   \n",
       "2         2                 Li1F1                  12.60             eV   \n",
       "3         3                 Li1F1                  12.10             eV   \n",
       "4         4                 Li1F1                  12.00             eV   \n",
       "...     ...                   ...                    ...            ...   \n",
       "1442   1454                 Th1O2                   3.30             eV   \n",
       "1443   1455                    UO                   1.50             eV   \n",
       "1444   1456                  U1O2                   2.18             eV   \n",
       "1445   1457                    UO                   0.60             eV   \n",
       "1446   1458                  U1O2                   1.30             eV   \n",
       "\n",
       "         Band gap method  Reliability  \n",
       "0             Reflection            1  \n",
       "1             Reflection            1  \n",
       "2              Estimated            2  \n",
       "3             Absorption            2  \n",
       "4             Absorption            2  \n",
       "...                  ...          ...  \n",
       "1442          Reflection            2  \n",
       "1443  Thermal activation            1  \n",
       "1444          Absorption            1  \n",
       "1445  Thermal activation            2  \n",
       "1446  Thermal activation            2  \n",
       "\n",
       "[1447 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mastml_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we dig into too much detail, lets take a second to understand what is included in the dataset column by column:\n",
    "\n",
    "1) Index   \n",
    "When dealing with large datasets having an explicit index is essential for keeping track of data points. Throughout the lab we'll be making changes to the dataset, and without proper indexing it's easy to make mistakes and lose track of where data came from.\n",
    "By Specifying a unique number to each datapoint we can always track things down to troubleshoot, make later changes, or track where something came from.\n",
    "\n",
    "2) chemicalFormula Clean  \n",
    "This is the key input parameter for all of the models you'll build. Fundamentally all of the information that the model contains can be represented by the chemical formulas in this column. \n",
    "Take a second to think about how powerful it would be to have a model that only has these simple letters and numbers as input. With an accurate model it would be possible to think of any composition of interest and obtain an almost immediate prediction.\n",
    "\n",
    "3) Band gap values Clean, Band gap units  \n",
    "Carrying on from the previous thought we have to ask ourselves \"what is it that we're predicting?\". In this case we have a dataset of band gap values for semiconductors and insulators. Knowledge of a material's bandgap is essential for a whole range of semiconductor applications.\n",
    "If we could predict a new material's band gap we could potentially accelerate discovery and design of materials, contributing to what is already more than a 400 billion dollar industry!\n",
    "\n",
    "4) Band gap method  \n",
    "We won't dive too deeply into the method information here directly, but notice that in the dataset we have a few different experimental measurement types. This is often the case when putting together large datasets that not all data is exactly equal. \n",
    "The accuracy of a model is often limited by the quality of data availabe so it's always important to understand where are the data comes from, and if it can be combined.\n",
    "\n",
    "5) Reliability  \n",
    "As a simpler version of the idea above about data quality we have a column labeled \"Reliability\". The researcher who put the dataset together took time to check each of their sources and come up with a reliability score or 1 or 2. \n",
    "A score of 1 indicates the most reliable data, and a 2 indicates that the samples may have been less pure, or the experimental technique was less accurate. As part of the data cleaning process later on we'll only use the most reliable data we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"exercise\">\n",
    "\n",
    "### Exercise 1.1<a name=\"e1.1\"></a>\n",
    "\n",
    "Question:  \n",
    "1. Look at the first 5 rows of this table. What do you notice about their chemical formula prevents us from this dataset as-is to train a machine learning model?  \n",
    "\n",
    "[Check or Submit Answers](#a1.1)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"lesson\">\n",
    "\n",
    "### Data cleaning<a name = \"lesson1.2\"></a>\n",
    "\n",
    "\n",
    "<div class = \"cellContent\">\n",
    "\n",
    "The bandgap dataset needs to be free of inconsistencies or ambiguities for machine learning to work properly. At this stage, the most important criteria include:\n",
    "1. Each set of inputs should only have one output. In our case, each material should only have one bandgap value.\n",
    "2. There are no missing input or output values.\n",
    "\n",
    "To tackle the first problem with duplicate data let's focus on the \"Reliability\" column to the far right.\n",
    "Notice that for many of the chemical formulas in the dataset we have multiple measurements, with some being labeled as more reliable and others as less reliable. For now let's filter out all the data points (rows in the dataset) that don't have a Reliability of 1, the most reliable label and see if that removed our duplicate data.\n",
    "\n",
    "this can be accomplished in the single line of code below.\n",
    "\n",
    "</div> <!-- closes content -->\n",
    "\n",
    "</div> <!-- closes lesson -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Filter for only Reliability 1\n",
    "mastml_df_filtered = mastml_df[mastml_df[\"Reliability\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>chemicalFormula Clean</th>\n",
       "      <th>Band gap values Clean</th>\n",
       "      <th>Band gap units</th>\n",
       "      <th>Band gap method</th>\n",
       "      <th>Reliability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Li1F1</td>\n",
       "      <td>13.600</td>\n",
       "      <td>eV</td>\n",
       "      <td>Reflection</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Li1F1</td>\n",
       "      <td>12.610</td>\n",
       "      <td>eV</td>\n",
       "      <td>Reflection</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Li1Cl1</td>\n",
       "      <td>9.330</td>\n",
       "      <td>eV</td>\n",
       "      <td>Reflection</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Li1Br1</td>\n",
       "      <td>7.950</td>\n",
       "      <td>eV</td>\n",
       "      <td>Absorption</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Li3Sb1</td>\n",
       "      <td>1.000</td>\n",
       "      <td>eV</td>\n",
       "      <td>Thermal activation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>1445</td>\n",
       "      <td>Bi</td>\n",
       "      <td>0.015</td>\n",
       "      <td>eV</td>\n",
       "      <td>Magnetoreflection</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1436</th>\n",
       "      <td>1448</td>\n",
       "      <td>Th1O2</td>\n",
       "      <td>5.750</td>\n",
       "      <td>eV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1437</th>\n",
       "      <td>1449</td>\n",
       "      <td>Th1O2</td>\n",
       "      <td>3.500</td>\n",
       "      <td>eV</td>\n",
       "      <td>Absorption</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1443</th>\n",
       "      <td>1455</td>\n",
       "      <td>UO</td>\n",
       "      <td>1.500</td>\n",
       "      <td>eV</td>\n",
       "      <td>Thermal activation</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1444</th>\n",
       "      <td>1456</td>\n",
       "      <td>U1O2</td>\n",
       "      <td>2.180</td>\n",
       "      <td>eV</td>\n",
       "      <td>Absorption</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>535 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index chemicalFormula Clean  Band gap values Clean Band gap units  \\\n",
       "0         0                 Li1F1                 13.600             eV   \n",
       "1         1                 Li1F1                 12.610             eV   \n",
       "6         6                Li1Cl1                  9.330             eV   \n",
       "7         7                Li1Br1                  7.950             eV   \n",
       "9         9                Li3Sb1                  1.000             eV   \n",
       "...     ...                   ...                    ...            ...   \n",
       "1433   1445                    Bi                  0.015             eV   \n",
       "1436   1448                 Th1O2                  5.750             eV   \n",
       "1437   1449                 Th1O2                  3.500             eV   \n",
       "1443   1455                    UO                  1.500             eV   \n",
       "1444   1456                  U1O2                  2.180             eV   \n",
       "\n",
       "         Band gap method  Reliability  \n",
       "0             Reflection            1  \n",
       "1             Reflection            1  \n",
       "6             Reflection            1  \n",
       "7             Absorption            1  \n",
       "9     Thermal activation            1  \n",
       "...                  ...          ...  \n",
       "1433   Magnetoreflection            1  \n",
       "1436                 NaN            1  \n",
       "1437          Absorption            1  \n",
       "1443  Thermal activation            1  \n",
       "1444          Absorption            1  \n",
       "\n",
       "[535 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print filtered data\n",
    "mastml_df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking through the filtered data and paying attention to the chemical formula column there are still some formulas for which we have multiple measurements. Because we don't have another way to decide which data points to keep, let's average the values between these multiple measurements.  \n",
    "\n",
    "To do this we'll use a method in Pandas (the dataframe package we are using to handle the data) called groupby which allows us to create groups of all of the identical formulas, and then average within each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Could not convert eVeV to numeric",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1490\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general.<locals>.array_func\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m   1489\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1490\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrouper\u001b[38;5;241m.\u001b[39m_cython_operation(\n\u001b[1;32m   1491\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maggregate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1492\u001b[0m         values,\n\u001b[1;32m   1493\u001b[0m         how,\n\u001b[1;32m   1494\u001b[0m         axis\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1495\u001b[0m         min_count\u001b[38;5;241m=\u001b[39mmin_count,\n\u001b[1;32m   1496\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1497\u001b[0m     )\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m   1499\u001b[0m     \u001b[38;5;66;03m# generally if we have numeric_only=False\u001b[39;00m\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;66;03m# and non-applicable functions\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m     \u001b[38;5;66;03m# try to python agg\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m     \u001b[38;5;66;03m# TODO: shouldn't min_count matter?\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/groupby/ops.py:959\u001b[0m, in \u001b[0;36mBaseGrouper._cython_operation\u001b[0;34m(self, kind, values, how, axis, min_count, **kwargs)\u001b[0m\n\u001b[1;32m    958\u001b[0m ngroups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngroups\n\u001b[0;32m--> 959\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cy_op\u001b[38;5;241m.\u001b[39mcython_operation(\n\u001b[1;32m    960\u001b[0m     values\u001b[38;5;241m=\u001b[39mvalues,\n\u001b[1;32m    961\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m    962\u001b[0m     min_count\u001b[38;5;241m=\u001b[39mmin_count,\n\u001b[1;32m    963\u001b[0m     comp_ids\u001b[38;5;241m=\u001b[39mids,\n\u001b[1;32m    964\u001b[0m     ngroups\u001b[38;5;241m=\u001b[39mngroups,\n\u001b[1;32m    965\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    966\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/groupby/ops.py:657\u001b[0m, in \u001b[0;36mWrappedCythonOp.cython_operation\u001b[0;34m(self, values, axis, min_count, comp_ids, ngroups, **kwargs)\u001b[0m\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ea_wrap_cython_operation(\n\u001b[1;32m    650\u001b[0m         values,\n\u001b[1;32m    651\u001b[0m         min_count\u001b[38;5;241m=\u001b[39mmin_count,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    655\u001b[0m     )\n\u001b[0;32m--> 657\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cython_op_ndim_compat(\n\u001b[1;32m    658\u001b[0m     values,\n\u001b[1;32m    659\u001b[0m     min_count\u001b[38;5;241m=\u001b[39mmin_count,\n\u001b[1;32m    660\u001b[0m     ngroups\u001b[38;5;241m=\u001b[39mngroups,\n\u001b[1;32m    661\u001b[0m     comp_ids\u001b[38;5;241m=\u001b[39mcomp_ids,\n\u001b[1;32m    662\u001b[0m     mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    663\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    664\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/groupby/ops.py:497\u001b[0m, in \u001b[0;36mWrappedCythonOp._cython_op_ndim_compat\u001b[0;34m(self, values, min_count, ngroups, comp_ids, mask, result_mask, **kwargs)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m--> 497\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_cython_op(\n\u001b[1;32m    498\u001b[0m     values,\n\u001b[1;32m    499\u001b[0m     min_count\u001b[38;5;241m=\u001b[39mmin_count,\n\u001b[1;32m    500\u001b[0m     ngroups\u001b[38;5;241m=\u001b[39mngroups,\n\u001b[1;32m    501\u001b[0m     comp_ids\u001b[38;5;241m=\u001b[39mcomp_ids,\n\u001b[1;32m    502\u001b[0m     mask\u001b[38;5;241m=\u001b[39mmask,\n\u001b[1;32m    503\u001b[0m     result_mask\u001b[38;5;241m=\u001b[39mresult_mask,\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    505\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/groupby/ops.py:541\u001b[0m, in \u001b[0;36mWrappedCythonOp._call_cython_op\u001b[0;34m(self, values, min_count, ngroups, comp_ids, mask, result_mask, **kwargs)\u001b[0m\n\u001b[1;32m    540\u001b[0m out_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_output_shape(ngroups, values)\n\u001b[0;32m--> 541\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_cython_function(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkind, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhow, values\u001b[38;5;241m.\u001b[39mdtype, is_numeric)\n\u001b[1;32m    542\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_cython_vals(values)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/groupby/ops.py:173\u001b[0m, in \u001b[0;36mWrappedCythonOp._get_cython_function\u001b[0;34m(cls, kind, how, dtype, is_numeric)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39m__signatures__:\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;66;03m# raise NotImplementedError here rather than TypeError later\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    174\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunction is not implemented for this dtype: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[how->\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,dtype->\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    176\u001b[0m     )\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: function is not implemented for this dtype: [how->mean,dtype->object]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/nanops.py:1692\u001b[0m, in \u001b[0;36m_ensure_numeric\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1691\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1692\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(x)\n\u001b[1;32m   1693\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1694\u001b[0m     \u001b[38;5;66;03m# e.g. \"1+1j\" or \"foo\"\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'eVeV'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/nanops.py:1696\u001b[0m, in \u001b[0;36m_ensure_numeric\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1696\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcomplex\u001b[39m(x)\n\u001b[1;32m   1697\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   1698\u001b[0m     \u001b[38;5;66;03m# e.g. \"foo\"\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: complex() arg is a malformed string",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mastml_df_clean \u001b[38;5;241m=\u001b[39m mastml_df_filtered\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchemicalFormula Clean\u001b[39m\u001b[38;5;124m\"\u001b[39m, as_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[1;32m      3\u001b[0m mastml_df_clean\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1855\u001b[0m, in \u001b[0;36mGroupBy.mean\u001b[0;34m(self, numeric_only, engine, engine_kwargs)\u001b[0m\n\u001b[1;32m   1853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_numba_agg_general(sliding_mean, engine_kwargs)\n\u001b[1;32m   1854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1855\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cython_agg_general(\n\u001b[1;32m   1856\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1857\u001b[0m         alt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: Series(x)\u001b[38;5;241m.\u001b[39mmean(numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only),\n\u001b[1;32m   1858\u001b[0m         numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only,\n\u001b[1;32m   1859\u001b[0m     )\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1507\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general\u001b[0;34m(self, how, alt, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[1;32m   1503\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agg_py_fallback(values, ndim\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mndim, alt\u001b[38;5;241m=\u001b[39malt)\n\u001b[1;32m   1505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m-> 1507\u001b[0m new_mgr \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mgrouped_reduce(array_func)\n\u001b[1;32m   1508\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_agged_manager(new_mgr)\n\u001b[1;32m   1509\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_aggregated_output(res)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/managers.py:1503\u001b[0m, in \u001b[0;36mBlockManager.grouped_reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mis_object:\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;66;03m# split on object-dtype blocks bc some columns may raise\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m     \u001b[38;5;66;03m#  while others do not.\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sb \u001b[38;5;129;01min\u001b[39;00m blk\u001b[38;5;241m.\u001b[39m_split():\n\u001b[0;32m-> 1503\u001b[0m         applied \u001b[38;5;241m=\u001b[39m sb\u001b[38;5;241m.\u001b[39mapply(func)\n\u001b[1;32m   1504\u001b[0m         result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/internals/blocks.py:329\u001b[0m, in \u001b[0;36mBlock.apply\u001b[0;34m(self, func, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[1;32m    325\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    326\u001b[0m \u001b[38;5;124;03m    apply the function to my values; return a block if we are not\u001b[39;00m\n\u001b[1;32m    327\u001b[0m \u001b[38;5;124;03m    one\u001b[39;00m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 329\u001b[0m     result \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_op_result(result)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1503\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general.<locals>.array_func\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m   1490\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrouper\u001b[38;5;241m.\u001b[39m_cython_operation(\n\u001b[1;32m   1491\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maggregate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1492\u001b[0m         values,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1496\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1497\u001b[0m     )\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m   1499\u001b[0m     \u001b[38;5;66;03m# generally if we have numeric_only=False\u001b[39;00m\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;66;03m# and non-applicable functions\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m     \u001b[38;5;66;03m# try to python agg\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m     \u001b[38;5;66;03m# TODO: shouldn't min_count matter?\u001b[39;00m\n\u001b[0;32m-> 1503\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agg_py_fallback(values, ndim\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mndim, alt\u001b[38;5;241m=\u001b[39malt)\n\u001b[1;32m   1505\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1457\u001b[0m, in \u001b[0;36mGroupBy._agg_py_fallback\u001b[0;34m(self, values, ndim, alt)\u001b[0m\n\u001b[1;32m   1452\u001b[0m     ser \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[:, \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1454\u001b[0m \u001b[38;5;66;03m# We do not get here with UDFs, so we know that our dtype\u001b[39;00m\n\u001b[1;32m   1455\u001b[0m \u001b[38;5;66;03m#  should always be preserved by the implemented aggregations\u001b[39;00m\n\u001b[1;32m   1456\u001b[0m \u001b[38;5;66;03m# TODO: Is this exactly right; see WrappedCythonOp get_result_dtype?\u001b[39;00m\n\u001b[0;32m-> 1457\u001b[0m res_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrouper\u001b[38;5;241m.\u001b[39magg_series(ser, alt, preserve_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1459\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, Categorical):\n\u001b[1;32m   1460\u001b[0m     \u001b[38;5;66;03m# Because we only get here with known dtype-preserving\u001b[39;00m\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;66;03m#  reductions, we cast back to Categorical.\u001b[39;00m\n\u001b[1;32m   1462\u001b[0m     \u001b[38;5;66;03m# TODO: if we ever get \"rank\" working, exclude it here.\u001b[39;00m\n\u001b[1;32m   1463\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(values)\u001b[38;5;241m.\u001b[39m_from_sequence(res_values, dtype\u001b[38;5;241m=\u001b[39mvalues\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/groupby/ops.py:994\u001b[0m, in \u001b[0;36mBaseGrouper.agg_series\u001b[0;34m(self, obj, func, preserve_dtype)\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(obj) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39m_values, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;66;03m# we can preserve a little bit more aggressively with EA dtype\u001b[39;00m\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;66;03m#  because maybe_cast_pointwise_result will do a try/except\u001b[39;00m\n\u001b[1;32m    990\u001b[0m     \u001b[38;5;66;03m#  with _from_sequence.  NB we are assuming here that _from_sequence\u001b[39;00m\n\u001b[1;32m    991\u001b[0m     \u001b[38;5;66;03m#  is sufficiently strict that it casts appropriately.\u001b[39;00m\n\u001b[1;32m    992\u001b[0m     preserve_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 994\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aggregate_series_pure_python(obj, func)\n\u001b[1;32m    996\u001b[0m npvalues \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmaybe_convert_objects(result, try_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    997\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preserve_dtype:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/groupby/ops.py:1015\u001b[0m, in \u001b[0;36mBaseGrouper._aggregate_series_pure_python\u001b[0;34m(self, obj, func)\u001b[0m\n\u001b[1;32m   1012\u001b[0m splitter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_splitter(obj, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(splitter):\n\u001b[0;32m-> 1015\u001b[0m     res \u001b[38;5;241m=\u001b[39m func(group)\n\u001b[1;32m   1016\u001b[0m     res \u001b[38;5;241m=\u001b[39m libreduction\u001b[38;5;241m.\u001b[39mextract_result(res)\n\u001b[1;32m   1018\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m initialized:\n\u001b[1;32m   1019\u001b[0m         \u001b[38;5;66;03m# We only do this validation on the first iteration\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1857\u001b[0m, in \u001b[0;36mGroupBy.mean.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1853\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_numba_agg_general(sliding_mean, engine_kwargs)\n\u001b[1;32m   1854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1855\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cython_agg_general(\n\u001b[1;32m   1856\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m-> 1857\u001b[0m         alt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: Series(x)\u001b[38;5;241m.\u001b[39mmean(numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only),\n\u001b[1;32m   1858\u001b[0m         numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only,\n\u001b[1;32m   1859\u001b[0m     )\n\u001b[1;32m   1860\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:11556\u001b[0m, in \u001b[0;36mNDFrame._add_numeric_operations.<locals>.mean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  11539\u001b[0m \u001b[38;5;129m@doc\u001b[39m(\n\u001b[1;32m  11540\u001b[0m     _num_doc,\n\u001b[1;32m  11541\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturn the mean of the values over the requested axis.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  11554\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m  11555\u001b[0m ):\n\u001b[0;32m> 11556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m NDFrame\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;28mself\u001b[39m, axis, skipna, numeric_only, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:11201\u001b[0m, in \u001b[0;36mNDFrame.mean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  11194\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[1;32m  11195\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  11196\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  11199\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m  11200\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m> 11201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stat_function(\n\u001b[1;32m  11202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, nanops\u001b[38;5;241m.\u001b[39mnanmean, axis, skipna, numeric_only, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m  11203\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:11158\u001b[0m, in \u001b[0;36mNDFrame._stat_function\u001b[0;34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  11154\u001b[0m     nv\u001b[38;5;241m.\u001b[39mvalidate_stat_func((), kwargs, fname\u001b[38;5;241m=\u001b[39mname)\n\u001b[1;32m  11156\u001b[0m validate_bool_kwarg(skipna, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskipna\u001b[39m\u001b[38;5;124m\"\u001b[39m, none_allowed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m> 11158\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reduce(\n\u001b[1;32m  11159\u001b[0m     func, name\u001b[38;5;241m=\u001b[39mname, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only\n\u001b[1;32m  11160\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:4670\u001b[0m, in \u001b[0;36mSeries._reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   4665\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   4666\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not allow \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwd_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumeric_only\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4667\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith non-numeric dtypes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4668\u001b[0m     )\n\u001b[1;32m   4669\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 4670\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op(delegate, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/nanops.py:96\u001b[0m, in \u001b[0;36mdisallow.__call__.<locals>._f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(invalid\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 96\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;66;03m# we want to transform an object array\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;66;03m# ValueError message to the more typical TypeError\u001b[39;00m\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# e.g. this is normally a disallowed function on\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# object arrays that contain strings\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_object_dtype(args[\u001b[38;5;241m0\u001b[39m]):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/nanops.py:158\u001b[0m, in \u001b[0;36mbottleneck_switch.__call__.<locals>.f\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    156\u001b[0m         result \u001b[38;5;241m=\u001b[39m alt(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 158\u001b[0m     result \u001b[38;5;241m=\u001b[39m alt(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/nanops.py:421\u001b[0m, in \u001b[0;36m_datetimelike_compat.<locals>.new_func\u001b[0;34m(values, axis, skipna, mask, **kwargs)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike \u001b[38;5;129;01mand\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    419\u001b[0m     mask \u001b[38;5;241m=\u001b[39m isna(values)\n\u001b[0;32m--> 421\u001b[0m result \u001b[38;5;241m=\u001b[39m func(values, axis\u001b[38;5;241m=\u001b[39maxis, skipna\u001b[38;5;241m=\u001b[39mskipna, mask\u001b[38;5;241m=\u001b[39mmask, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    423\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike:\n\u001b[1;32m    424\u001b[0m     result \u001b[38;5;241m=\u001b[39m _wrap_results(result, orig_values\u001b[38;5;241m.\u001b[39mdtype, fill_value\u001b[38;5;241m=\u001b[39miNaT)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/nanops.py:727\u001b[0m, in \u001b[0;36mnanmean\u001b[0;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[1;32m    724\u001b[0m     dtype_count \u001b[38;5;241m=\u001b[39m dtype\n\u001b[1;32m    726\u001b[0m count \u001b[38;5;241m=\u001b[39m _get_counts(values\u001b[38;5;241m.\u001b[39mshape, mask, axis, dtype\u001b[38;5;241m=\u001b[39mdtype_count)\n\u001b[0;32m--> 727\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m _ensure_numeric(values\u001b[38;5;241m.\u001b[39msum(axis, dtype\u001b[38;5;241m=\u001b[39mdtype_sum))\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(the_sum, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    730\u001b[0m     count \u001b[38;5;241m=\u001b[39m cast(np\u001b[38;5;241m.\u001b[39mndarray, count)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/nanops.py:1699\u001b[0m, in \u001b[0;36m_ensure_numeric\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1696\u001b[0m             x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcomplex\u001b[39m(x)\n\u001b[1;32m   1697\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m   1698\u001b[0m             \u001b[38;5;66;03m# e.g. \"foo\"\u001b[39;00m\n\u001b[0;32m-> 1699\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not convert \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to numeric\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not convert eVeV to numeric"
     ]
    }
   ],
   "source": [
    "mastml_df_clean = mastml_df_filtered.groupby(\"chemicalFormula Clean\", as_index = False).mean()\n",
    "\n",
    "mastml_df_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"exercise\">\n",
    "\n",
    "### Exercise 1.2<a name=\"e1.2\"></a>\n",
    "\n",
    "Question:  \n",
    "    1. Look at the starting dataframe mastml_df, How Many data points did we start with? (make sure to look at the very first dataframe we imported and not the one after filtering for reliability) \n",
    "    2. Now look at the cleaned dataframe mastml_df_clean, how many data points do we have now?  \n",
    "\n",
    "[Check or Submit Answers](#a1.2)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"lesson\">\n",
    "\n",
    "### Evaluating Data availability<a name = \"lesson1.3\"></a>\n",
    "\n",
    "<div class = \"cellContent\">\n",
    "\n",
    "In general, the more data we have, the more confident we are at predicting properties of materials that are similar to our training data.\n",
    "\n",
    "Now our dataset has been cleaned, let's start to look in more detail at the band gap values that we have. When preparing to work with a dataset, it's important to know what the values trying to be predicted look like. What's their scale and how are they distributed.\n",
    "\n",
    "</div> <!-- closes content -->\n",
    "\n",
    "</div> <!-- closes lesson -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate basic statistics on our band gap values\n",
    "mastml_df_clean[\"Band gap values Clean\"].describe().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"exercise\">\n",
    "\n",
    "### Exercise 1.3<a name=\"e1.3\"></a>\n",
    "\n",
    "Question:  \n",
    "    1. What is the range of band gap values?  \n",
    "    2. Think ahead to once we build our model. Would a predicted error of 5 eV be considered small enough to be an accurate or useful prediction?  \n",
    "    3. How about a predicted error of 0.5 eV?\n",
    "\n",
    "[Check or Submit Answers](#a1.3)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from just the ranges of values it is also useful to visualize the distribution of data.  \n",
    "Let's build a simple histogram of the band gap values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll also define a simple histogram plotting function to use later\n",
    "def histogram_plot(data):\n",
    "    fig1,ax1 = plt.subplots()\n",
    "    ax1.hist(data,bins=range(13),density=1)\n",
    "    ax1.set_xticks(range(13))\n",
    "    ax1.set_xlabel('Measured Bandgap [eV]')\n",
    "    ax1.set_ylabel('Counts [fraction]')\n",
    "    plt.show()\n",
    "\n",
    "histogram_plot(mastml_df_clean[\"Band gap values Clean\"].astype(\"float\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"exercise\">\n",
    "\n",
    "### Exercise 1.4<a name=\"e1.4\"></a>\n",
    "\n",
    "Questions:  \n",
    "1. Is our band gap data balanced (i.e. uniformly distributed across its range)?\n",
    "2. Given your answer to question 1, Would you expect that the model has similar performance between 0-2 eV as between 10-12 eV?\n",
    "\n",
    "Challenges (optional questions):\n",
    "3. What is one thing we can do to address this issue?\n",
    "\n",
    "[Check or Submit Answers](#a1.4)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also try to get a feel for the compositions present in our dataset.  \n",
    "Specifically we'll focus on looking at which elements are present in the data, and in what quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# parse out individual elements for each formula using pymatgen's composition parser\n",
    "element_list = list()\n",
    "for idx in mastml_df.index:\n",
    "    element_list.extend(pymatgen.core.composition.Composition(mastml_df[\"chemicalFormula Clean\"][idx]).elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# setup a counter to count each element\n",
    "temp_counter = Counter(element_list)\n",
    "element_tuples = list(zip(list(temp_counter.keys()),list(temp_counter.values())))\n",
    "element_df = pd.DataFrame(element_tuples,columns=[\"Element\",\"Count\"])\n",
    "element_df_sorted = element_df.sort_values(by=[\"Count\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "element_df_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"exercise\">\n",
    "\n",
    "### Exercise 1.5<a name=\"e1.5\"></a>\n",
    "Questions:  \n",
    "1. What are the five most common elements in the dataset?  \n",
    "2. What are the five least common elements in the dataset?  \n",
    "3. Rank your confidence in the following predictions:  \n",
    "\n",
    "    - predictions containing Oxygen (oxides)  \n",
    "    - predictions containing Iridium  \n",
    "    - predictions containing an element that doesn't appear in the dataset at all  \n",
    "\n",
    "[Check or Submit Answers](#a1.5)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output data to csv - note depending on when you run this the updated data file may have been pregenerated so this cell isn't technically necessary.\n",
    "output_path = \"../data/bandgap_data_v3.csv\"\n",
    "\n",
    "if os.path.isfile(output_path):\n",
    "    print(output_path,\" exists, not creating new file\")\n",
    "else:\n",
    "    mastml_df_clean.to_csv(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"section\">\n",
    "\n",
    "# 2. Feature Generation<a name=\"2\"></a>\n",
    "---\n",
    "[Back to TOC](#toc)\n",
    "\n",
    "<div class = \"cellContent\">\n",
    "\n",
    "## Overview\n",
    "This section covers generating features from chemical formulas in the dataset. This featurization step is necessary to turn the text representation of materials into a numerical representation that the models can understand! In order to make this notebook independent from outside code versioning as much as possible, we'll use pre-generated features using the MAST-ML code package instead of generating them in real time in this notebook. Though the features used are from a previous version of MAST-ML which generated a list of \"composition average elemental properties\" to represent each unique material.\n",
    "\n",
    "## Learning Outcomes\n",
    "1. Validate generated features by manual calculation\n",
    "2. Assess generated features using criteria of variability\n",
    "3. Inspect features for relative size and shape\n",
    "3. Discuss qualities of a good feature\n",
    "\n",
    "\n",
    "</div> <!-- closes content -->\n",
    "\n",
    "</div> <!-- closes section -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"lesson\">\n",
    "\n",
    "### MASTML Configuration<a name = \"lesson2.1\"></a>\n",
    "\n",
    "<div class = \"cellContent\">\n",
    "\n",
    "[Materials Simulation Toolkit for Machine Learning (MAST-ML)](https://mastmldocs.readthedocs.io/en/latest/) is an open-source Python package designed to broaden and accelerate the use of machine learning in materials science research. It supports a full end-to-end machine learning workflow including all of the steps that we code by hand in this lab, but here we will talk through how specifically it generates Composition Average Elemental Properties to featurize a dataset. \n",
    "\n",
    "</div> <!-- closes content -->\n",
    "\n",
    "</div> <!-- closes lesson -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a new dataframe of generated features from the pregenerated matml run.\n",
    "cwd = os.getcwd()\n",
    "generated_features_path = os.path.join(cwd,\"./generated_features/generated_features.csv\")\n",
    "features_df = pd.read_csv(generated_features_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw MASTML output combines the original data and the generated features in one single dataframe, which isn't ideal. To make our next step (feature engineering) easier, We will split it into two dataframes:\n",
    "1. `target_data_df`: target values (outputs)\n",
    "2. `features_df`: features (inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split features_df into two dataframes\n",
    "target_data_df = pd.DataFrame([features_df[\"chemicalFormula Clean\"],features_df[\"Band gap values Clean\"],features_df[\"Band gap units\"],features_df[\"index\"],features_df[\"Reliability\"]]).T\n",
    "features_df = features_df.drop(columns=['index','Reliability','Band gap values Clean','Band gap units','chemicalFormula Clean'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a look at our target values first. Note that it still contains other input information (such as chemical formula) to help you contextualize what the bandgap values mean. Later, we will drop these columns as they won't be used in the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_data_df # our original dataset with inputs and outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also take a look at the features generated. Looking at the column names you will notice that each of them follows the pattern of: ElementalProperty_composition_average\n",
    "\n",
    "Some of these properties may be familiar to you such as AtomicWeight, which can be looked up in the periodic table of the elements. Others may be a bit harder to understand from their shorthand such as BCCefflatcnt, which stands for Body Centered Cubic effective lattice constant. In this case this property is describing information about how long certain bond lengths are within an idealized crystal of the element. Even though they are more complex they have still be tabulated by previous researchers and therefore MAST-ML is able to simply look them up from known resources to calculate the properties shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_df # features generated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"exercise\">\n",
    "\n",
    "### Exercise 2.1<a name=\"e2.1\"></a>\n",
    "Questions:  \n",
    "1. How many features have we generated?  \n",
    "\n",
    "[Check or Submit Answers](#a2.1)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"lesson\">\n",
    "\n",
    "### Validating compositional average features<a name = \"lesson2.2\"></a>\n",
    "\n",
    "<div class = \"cellContent\">\n",
    "\n",
    "The Features that we've generated are all compositional averages of elemental properties. Using an existing database of elemental properties, the MASTML code reads in each of the chemical formulas in the dataset and combines each elemental property for each element in the formula according to the following equation:  \n",
    "  \n",
    "\\begin{equation}\n",
    "Property\\_CompositionAverage = \\frac{A \\cdot Property\\_Value_A + B \\cdot Property\\_Value_B}{A + B}\n",
    "\\end{equation}\n",
    "  \n",
    "Where A and B are the amounts of each element in the formula.\n",
    "\n",
    "</div> <!-- closes content -->\n",
    "\n",
    "</div> <!-- closes lesson -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"exercise\">\n",
    "\n",
    "### Exercise 2.2<a name=\"e2.2\"></a>\n",
    "Questions:  \n",
    "1. Calculate the $AtomicNumber\\_CompositionAverage$ for the chemical Formula $\\text{Li}_3\\text{Sb}_1. (AtomicNumber_\\text{Li} = 3, AtomicNumber_\\text{Sb} = 51)$\n",
    "\n",
    "[Check or Submit Answers](#a2.2)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"section\">\n",
    "\n",
    "# 3. Feature Engineering<a name=\"3\"></a>\n",
    "---\n",
    "[Back to TOC](#toc)\n",
    "\n",
    "<div class = \"cellContent\">\n",
    "\n",
    "## Overview\n",
    "The next big step we need to do before building models and making prediction is to make sure our features are useful for modeling. We'll perform three checks on our features that should improve their usefulness. This step is similar to the initial dataset cleaning / pre-processing that we did at the start of the lab, except now we're focusing on \"cleaning\" our features instead of cleaning the output data and compositions.\n",
    "\n",
    "Feature Engineering Steps:  \n",
    "1) Remove Constant Columns  \n",
    "2) Remove Highly Correlated Columns  \n",
    "3) Normalize Features  \n",
    "\n",
    "\n",
    "## Learning Outcomes\n",
    "1. Explain benefits of feature normalization, removing correlated features, and removing constant features\n",
    "2. Recall that good features should be able provide information about the target variable\n",
    "3. Recall that good features provide unique information about the target variable\n",
    "4. Define a basic normalization scheme\n",
    "5. Execute a normalization scheme\n",
    "\n",
    "\n",
    "</div> <!-- closes content -->\n",
    "\n",
    "</div> <!-- closes section -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"lesson\">\n",
    "\n",
    "### Remove constant columns<a name = \"lesson3.1\"></a>\n",
    "\n",
    "<div class = \"cellContent\">\n",
    "\n",
    "Columns that have a constant value for all data points have no information and can't possibly help the ML model learn anything.  \n",
    "We'll throw those out.\n",
    "\n",
    "</div> <!-- closes content -->\n",
    "\n",
    "</div> <!-- closes lesson -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Constant Columns\n",
    "features_df_noconstant = features_df.loc[:, (features_df != features_df.iloc[0]).any()] \n",
    "\n",
    "# report number of columns\n",
    "len(features_df_noconstant.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"exercise\">\n",
    "\n",
    "### Exercise 3.1<a name=\"e3.1\"></a>\n",
    "Questions:  \n",
    "1. How many features do we have left?  \n",
    "2. Should you worry about having too few useful features? \n",
    "\n",
    "[Check or Submit Answers](#a3.1)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"lesson\">\n",
    "\n",
    "### remove highly correlated features<a name = \"lesson3.2\"></a>\n",
    "\n",
    "<div class = \"cellContent\">\n",
    "\n",
    "Features that are extremely similar don't give any additional information beyond the first appearance.  \n",
    "They can also confuse the model by giving very similar information that may essentially overload the model.  \n",
    "Before we remove features, let's calculate the `correlation matrix` for all the features. \n",
    "\n",
    "</div> <!-- closes content -->\n",
    "\n",
    "</div> <!-- closes lesson -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Highly correlated Features\n",
    "# using notes here for methodology: https://chrisalbon.com/machine_learning/feature_selection/drop_highly_correlated_features/\n",
    "\n",
    "features_corr_df = features_df_noconstant.corr(method=\"pearson\").abs()\n",
    "features_corr_df.iloc[:5, :5] # Preview the first 5 rows/columns of the correlation matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A better way to interpret this correlation matrix is by plotting a heatmap: The darker the color on the plot, the more highly correlated features are.   \n",
    "Note the diagonal line with all 1 values. This is because each feature is by definition perfectly correlated with itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# before removing correlated features\n",
    "fig1, ax1 = plt.subplots(figsize=(10,5))\n",
    "c = ax1.pcolor(features_corr_df,cmap=\"Blues\")\n",
    "ax1.set_ylim(ax1.get_ylim()[::-1])\n",
    "ax1.xaxis.set_ticks_position('top')\n",
    "ax1.xaxis.set_label_position('top')\n",
    "ax1.set_xlabel('Feature Numbers')\n",
    "ax1.set_ylabel('Feature Numbers')\n",
    "ax1.set_aspect('equal')\n",
    "plt.colorbar(c,ax=ax1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the features with correlation coefficients above 0.95\n",
    "upper = features_corr_df.where(np.triu(np.ones(features_corr_df.shape), k=1).astype(np.bool))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "features_df_lowcorr = features_df_noconstant.drop(columns=to_drop)\n",
    "# recalculate the correlation matrix so we can compare\n",
    "features_corr_df_update = features_df_lowcorr.corr(method=\"pearson\").abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot correlation after removing highly correlated features\n",
    "\n",
    "fig1, (ax1,ax2) = plt.subplots(1,2,figsize=(15,5))\n",
    "c1 = ax1.pcolor(features_corr_df,cmap=\"Blues\")\n",
    "ax1.set_ylim(ax1.get_ylim()[::-1])\n",
    "ax1.xaxis.set_ticks_position('top')\n",
    "ax1.xaxis.set_label_position('top')\n",
    "ax1.set_xlabel('Feature Numbers')\n",
    "ax1.set_ylabel('Feature Numbers')\n",
    "ax1.set_aspect('equal')\n",
    "\n",
    "plt.colorbar(c1,ax=ax1)\n",
    "\n",
    "c2 = ax2.pcolor(features_corr_df_update,cmap=\"Blues\")\n",
    "ax2.set_ylim(ax2.get_ylim()[::-1])\n",
    "ax2.xaxis.set_ticks_position('top')\n",
    "ax2.xaxis.set_label_position('top')\n",
    "ax2.set_xlabel('Feature Numbers')\n",
    "ax2.set_ylabel('Feature Numbers')\n",
    "ax2.set_aspect('equal')\n",
    "plt.colorbar(c2,ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features_df_lowcorr.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"exercise\">\n",
    "\n",
    "### Exercise 3.2<a name=\"e3.2\"></a>\n",
    "Questions:  \n",
    "1. After filtering for highly correlated features how many features do we have left?  \n",
    "2. Are we worried about having too few useful features? \n",
    "\n",
    "[Check or Submit Answers](#a3.2)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"lesson\">\n",
    "\n",
    "### feature normalization<a name = \"lesson3.3\"></a>\n",
    "\n",
    "<div class = \"cellContent\">\n",
    "\n",
    "Finally the last thing that we need to do is perform some normalization or rescaling of our features.  \n",
    "Lets take a look at the updated feature dataset and pay attention to the types of features, their ranges, and scale similar to what we did for the band gap earlier\n",
    "\n",
    "</div> <!-- closes content -->\n",
    "\n",
    "</div> <!-- closes lesson -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features_df_lowcorr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be fairly apparent that our features come in many shapes and sizes. Machine Learning algorithms can be very sensitive to these differences.  \n",
    "For example one feature may be several orders of magnitude larger in values and in range of values.  \n",
    "This can make some algorithms significantly biased towards those features so the best practice is usually to perform some alteration to make all the features look similar, while still preserving the information they contain\n",
    "\n",
    "In our case we're going to linearly rescale the features so that they all have the same minimum and same maximum. If you're interested in checking of the details of how this is done you can check out the documentation for the Scikit-learn method we'll be using: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_features = MinMaxScaler().fit_transform(features_df_lowcorr)\n",
    "minmax_features_df = pd.DataFrame(minmax_features,columns=features_df_lowcorr.columns)\n",
    "minmax_features_df.iloc[:5, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue> challenge: Turn off the feature normalization. Feature normalization is a common practice to enable models to better learn from multiple features when some are on significantly different scales. \n",
    "Try removing this section to see how the later results are affected. In the case of the decision tree / random forest model being used by default this may not be the case, but what about other model types? Try doing the same thing with a Kernel Ridge Regression Model for example. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how compared to some of the previous sections, performing the scaling only took a few lines of code. This is the power of using existing code packages and tools that are already out there!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"section\">\n",
    "\n",
    "# 4. Setup for Model Evaluation<a name=\"4\"></a>\n",
    "---\n",
    "[Back to TOC](#toc)\n",
    "\n",
    "<div class = \"cellContent\">\n",
    "\n",
    "## Overview\n",
    "Before jumping in to model building, we have one last thing to think about: How are we going to know how well our models are performing?  \n",
    "\n",
    "We can test the model by asking it to predict the bandgap value of materials, compare it with measured values, and calculate its prediction error using a range of error metrics (e.g. RMSE, R2, etc), but the naive approach of training and testing the model using the same dataset gives us a <u>biased estimate</u> of model error because the model has *seen* all the data we are asking it to predict already.  \n",
    "\n",
    "To get an <u>unbiased estimate</u> of model error, we can employ a simple <u>cross-validation</u> technique: Setting aside a subset of data as our <u>test set</u>, which the model won't access during the training process. The rest of the data constitutes the <u>training set</u>, which is used to train the model. This allows us to then evaluate model performance by comparing model prediction on both seen data (training set) and unseen data (test set).\n",
    "\n",
    "## Learning Outcomes\n",
    "1. Explain how model evaluation is tied directly to ML application\n",
    "2. Propose a train test split to only predict high bandgap materials\n",
    "3. Define how training and test splits are used by a model\n",
    "\n",
    "\n",
    "</div> <!-- closes content -->\n",
    "\n",
    "</div> <!-- closes section -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"lesson\">\n",
    "\n",
    "### Establishing train/test split<a name = \"lesson4.1\"></a>\n",
    "\n",
    "<div class = \"cellContent\">\n",
    "    \n",
    "For now lets split off 10% of the data for testing.\n",
    "\n",
    "</div> <!-- closes content -->\n",
    "\n",
    "</div> <!-- closes lesson -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we store our cleaned and normalized inputs and outputs in new variables `X` and `y` for easier understanding and manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = minmax_features_df                         # inputs/features \n",
    "y = target_data_df[\"Band gap values Clean\"] # outputs/targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, if you are using machine learning to predict the bandgap (or other properties) of a novel material, you won't know its real bandgap until you fabricate and measure it in the lab, which is bad news for instructors: What's the point if we weren't able to validate the predictions and show you the power of machine learning?   \n",
    "\n",
    "Therefore for instructional purposes, we will stage our prediction by using 5 common materials with known bandgap values instead - Si, SiO2, C, NaCl, and Sn - and removing them from the dataset.\n",
    "\n",
    "The following code accomplishes the above and is not important otherwise. Note that you do **NOT** need this step in a real research setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find prediction compounds and generate inputs for them to make predictions later.\n",
    "def extract_predictions(formula=\"string\"):\n",
    "    index_prediction = target_data_df[target_data_df[\"chemicalFormula Clean\"]==formula].index\n",
    "    xpredict = X.loc[index_prediction].copy()\n",
    "    ypredict = y.loc[index_prediction]\n",
    "    \n",
    "    return (index_prediction,xpredict,ypredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_predict_Si, xpredict_Si, ypredict_Si = extract_predictions(formula=\"Si\")\n",
    "index_predict_SiO2, xpredict_SiO2, ypredict_SiO2 = extract_predictions(formula=\"Si1O2\")\n",
    "index_predict_C, xpredict_C, ypredict_C = extract_predictions(formula=\"C\")\n",
    "index_predict_Sn, xpredict_Sn, ypredict_Sn = extract_predictions(formula=\"Sn\")\n",
    "index_predict_NaCl, xpredict_NaCl, ypredict_NaCl = extract_predictions(formula=\"Na1Cl1\")\n",
    "\n",
    "X_predict = X.drop(index=index_predict_Si.to_list()+index_predict_SiO2.to_list()+index_predict_C.to_list()+index_predict_Sn.to_list()+index_predict_NaCl.to_list())\n",
    "y_predict = y.drop(index=index_predict_Si.to_list()+index_predict_SiO2.to_list()+index_predict_C.to_list()+index_predict_Sn.to_list()+index_predict_NaCl.to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the `train_test_split()` method from the `scikit-learn` package to generate the split. In this case, our input data `X` and output data `y` are split into 4 parts:    \n",
    "- `X_train`: training set input data  \n",
    "- `X_test`: test set input data  \n",
    "- `y_train`: training set output data  \n",
    "- `y_test`: test set output data  \n",
    "\n",
    "We will continue referencing these 4 objects throughout the rest of this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate train/test split by reserving 10% of data as test set\n",
    "\n",
    "test_fraction = 0.1\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_fraction, shuffle=True,random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"lesson\">\n",
    "\n",
    "### Evaluating train/test split<a name = \"lesson4.2\"></a>\n",
    "\n",
    "<div class = \"cellContent\">\n",
    "    \n",
    "One thing that can be interesting to check is how \"representative\" the splits are of the full dataset. In an ideal world we'd have enough data that even splitting out 10% of it would have enough data in each set to accurately reproduce the whole dataset.  \n",
    "\n",
    "Below lets plot the same histogram of band gap values as we did before, and then plot the same for the train and test set.\n",
    "\n",
    "</div> <!-- closes content -->\n",
    "\n",
    "</div> <!-- closes lesson -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(2, figsize=(10,5), sharex = True, gridspec_kw={'hspace': 0})\n",
    "fig.set_tight_layout(False)\n",
    "myarray = mastml_df_clean[\"Band gap values Clean\"]\n",
    "\n",
    "bins = np.true_divide(range(28),2)\n",
    "\n",
    "l1 = sns.distplot(y_train.astype(\"float\"), hist = True, norm_hist = True, kde = False, bins = bins, hist_kws={\"edgecolor\": \"white\"}, label = 'training set', ax = ax1)\n",
    "l2 = sns.distplot(y_test.astype(\"float\"), hist = True, norm_hist = True, kde = False, bins = bins, hist_kws={\"edgecolor\": \"white\", \"color\": \"orange\"}, label = 'test set', ax = ax2)\n",
    "l3 = sns.distplot(myarray, hist = True, norm_hist = True, kde = False, bins = bins, hist_kws={\"histtype\": \"step\",\"linewidth\": 3, \"alpha\": 1, \"color\": \"grey\"}, ax = ax1)\n",
    "l4 = sns.distplot(myarray, hist = True, norm_hist = True, kde = False, bins = bins, hist_kws={\"histtype\": \"step\",\"linewidth\": 3, \"alpha\": 1, \"color\": \"grey\"}, label = 'full dataset', ax = ax2)\n",
    "\n",
    "\n",
    "ax1.set_xticks(range(14))\n",
    "ax2.set_xticks(range(14))\n",
    "ax2.xaxis.label.set_visible(False)\n",
    "handles, labels = [(a + b) for a, b in zip(ax1.get_legend_handles_labels(), ax2.get_legend_handles_labels())]\n",
    "fig.suptitle('Comparing histograms of the train/test split')\n",
    "fig.add_subplot(111, frame_on=False)\n",
    "plt.tick_params(labelcolor=\"none\", bottom=False, left=False)\n",
    "plt.legend(handles, labels, loc = 'center left', bbox_to_anchor=(1, 0.5),prop={'size': 16})\n",
    "plt.xlabel('Measured Bandgap (eV)')\n",
    "_ = plt.ylabel('Density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"exercise\">\n",
    "\n",
    "### Exercise 4.1<a name=\"e4.1\"></a>\n",
    "Questions:  \n",
    "1. Does it look like we have enough data that the test split is identical to the full dataset and the train split?\n",
    "\n",
    "[Check Answers](#a4.1)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answers:  \n",
    "1.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"section\">\n",
    "\n",
    "# 5. Fitting and Evaluating a Default Model<a name=\"5\"></a>\n",
    "---\n",
    "[Back to TOC](#toc)\n",
    "\n",
    "<div class = \"cellContent\">\n",
    "\n",
    "## Overview\n",
    "\n",
    "Significance:\n",
    "Status quo: what did we do in last section\n",
    "    So far, our dataset has been inspected, properly cleaned and split into training and test sets.\n",
    "Gap: \n",
    "Fill gap: \n",
    "\n",
    "what you will do: Now we finally get to the fun part: Building models and make predictions! We're going to fit a decision tree model to the training data we created in the previous section. Then we'll use that model to predict both the test data and the training data.\n",
    "\n",
    "\n",
    "## Learning Outcomes\n",
    "By the end of this section, you will be able to:\n",
    "1. Evaluate model performance\n",
    "    1. Analyze parity plots for qualitative performance of models\n",
    "    2. Interpret error metrics to compare model performance\n",
    "2. Recognize value of test data predictions in assessing predictive ability compared to training data\n",
    "3. Identify key indicators of overfitting\n",
    "4. Estimate model performance for two different ML applications\n",
    "\n",
    "\n",
    "</div> <!-- closes content -->\n",
    "\n",
    "</div> <!-- closes section -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"lesson\">\n",
    "\n",
    "### Fitting the decision tree model<a name = \"lesson5.1\"></a>\n",
    "\n",
    "<div class = \"cellContent\">\n",
    "\n",
    "We'll train or \"fit\" a decision tree model using the `RandomForestRegressor().fit()` class from the `scikit-learn` package we imported at the beginning of the notebook. Notice how this only takes 1 single line of code because we're leveraging existing code package that's been built for us already. Also note that the actual class name that we're using is called RandomForestRegressor. This RandomForestRegressor class can be configured to be identical to a decision tree, however the reason we'll use it is because later on it will give us more flexibility when we start changing the model. A decision tree model is basically a random forest model with 1 tree, so we'll set the `n_estimators` hyperparameter to 1 to make the model mimic a single decision tree.\n",
    "\n",
    "Keep in mind that machine learning is much more than this 1 line of code: You have already gone through a ton of preprocessing and decision making to reach this step!\n",
    "\n",
    "</div> <!-- closes content -->\n",
    "\n",
    "</div> <!-- closes lesson -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Default_model = RandomForestRegressor(random_state=seed,n_estimators=1,bootstrap=False).fit(X_train,y_train) # fit the decision tree model\n",
    "print('Model training complete.')\n",
    "# print('Tree depth:', [estimator.tree_.max_depth for estimator in Default_model.estimators_])\n",
    "# for importance in zip (estimator.feature_importances_ for estimator in Default_model.estimators_):\n",
    "#     print (importance)\n",
    "#print('Leaf nodes:',[estimator.tree_.n_leaves for estimator in Default_model.estimators_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs above describes the <u>hyperparameters</u> selected (in this case, by default) to fit the decision tree model.  and the <u>parameters</u> being generated in the training process. You may also wonder what the decision tree *looks* like, and we will visualize the entire tree later.  \n",
    "\n",
    "We'll also go into more detail about what these (hyper)parameters are at a later time when they become more relevant. For now, we're glossing over because simply knowing these (hyper)parameters doesn't help us evaluate model quality until we have seen its performance: How accurately and precisely can our decision tree model predict bandgaps? We will jump into that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As one last motivation as we start asssessing our model, lets predict two band gaps of materials you're probably familiar with, Silicon and Silica. Silicon is used in practically every electronic device as a semiconductor, and Silica is basic window glass. You can look up the values of their band gaps fro reference, but look how just in a few lines of code the model can already give us a rough idea of their values. We know Silicon is a semi-conductor and it's bandgap should be fairly low, while the band gap for Silica has to be much higher because window glass shouldn't absorb any light at all. Based on these predictions it seems like the model can already pick up on these trends! But as we've been mentioning, just making a few select predictions is not a good way to measure overall performance, in the next sections we'll dig into more robus ways to measure the performance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Default_model_all_data = RandomForestRegressor(random_state=seed,n_estimators=1,bootstrap=False).fit(X_predict,y_predict)\n",
    "\n",
    "print(\"Predicting Silicon Band Gap: \",Default_model_all_data.predict(xpredict_Si))\n",
    "\n",
    "print(\"Predicting Silica Band Gap: \",Default_model_all_data.predict(xpredict_SiO2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"lesson\">\n",
    "\n",
    "### Evaluating model performance on training and test data<a name = \"lesson5.2\"></a>\n",
    "\n",
    "<div class = \"cellContent\">\n",
    "\n",
    "To evaluate the model performance, we will use it to predict bandgaps of materials it was trained on (training set) as well as materials it has not seen before (test set), and we will compare its performance on both datasets.\n",
    "\n",
    "There are two main ways to understand and evaluate prediction performance:\n",
    "1. Qualitatively, we can create a scatter plot with predicted values on the y-axis and the actual measured values on the x-axis, known as <u>parity plots</u> or <u>predicted versus actual plots</u>. The more closely aligned the data points are to the ideal diagonal line, where each prediction matches the measured value perfectly, the better the model performs on that specific dataset. Parity plots also surface outliers, trends and patterns in the prediction results.\n",
    "2. Quantitatively, we can measure model performance using statistical error metrics. Some of these error metrics, such as RMSE and R2, will be familiar to you if you have taken an introductory statistics class.\n",
    "\n",
    "</div> <!-- closes content -->\n",
    "\n",
    "</div> <!-- closes lesson -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_predictions = Default_model.predict(X_train)    # Make predictions on training data\n",
    "Test_predictions = Default_model.predict(X_test)      # Make predictions on testing data\n",
    "\n",
    "parity_plots_side_by_side(y_train,Train_predictions,y_test,Test_predictions,title_left=\"Training Data Parity Plot\",title_right=\"Test Data Parity Plot\") # build both plots\n",
    "parity_stats_side_by_side(y_train,Train_predictions,y_test,Test_predictions,\"Training Data\",\"Test Data\")  # print error metrics for training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've generated a few different error metrics which we can use to asses the model's performance. One that we'll focus on throughout the lab is the Root Mean Squared Error (RMSE), which we are going to use as a rough error bar when talking about predictive ability of the model. Meaning when we're analyzing performance and asking questions about how accurate the model is is making predictions this is the statistic we'll reference. It's important to note that this is just one choice we could make for assigning an error bar to the model's predictions. There are other, more complex methods for generating error bars for model predictions, and we're going to ignore those for now in favor of simplicity. So whenever we're asking you to think about the predictive power of the model for now think of model predictions as having a predicted value plus or minus the RMSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"exercise\">\n",
    "\n",
    "### Exercise 5.1<a name=\"e5.1\"></a>\n",
    "Questions:  \n",
    "1. Take a look at the training parity plot. Evaluate model prediction on training data by answering the following:  \n",
    "    1. Is there enough information in the features to make predictions? Do the features model this data (training) well?\n",
    "    2. Are there any outliers?  \n",
    "    3. Does it consistently overpredict/underpredict bandgap values in any particular range?  \n",
    "\n",
    "Take a look at all the training data statistics (RMSE, RMSE/std, MAE, R2). Assume these metrics, specifically RMSE, can be used to put an error bar on any predictions we make. Recall that we identified two ML applications for this model, and answer:\n",
    "\n",
    "2. Can we use this model to predict bandgap values of materials for making single-junction solar cell, which requires a bandgap between 1.1 and 1.7eV?\n",
    "3. Can we use this model to predict high bandgap materials above 3 eV?\n",
    "\n",
    "[Check or Submit Answers](#a5.1)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"exercise\">\n",
    "\n",
    "### Exercise 5.2<a name=\"e5.2\"></a>\n",
    "Questions:  \n",
    "1. Take a look at the test parity plot. Evaluate model prediction on test data by answering the following:  \n",
    "    1. Is there enough information in the features to make predictions?\n",
    "    2. Are there any outliers?  \n",
    "    3. Does it consistently overpredict/underpredict bandgap values in any particular range? \n",
    "2. Take a look at all the test data statistics (RMSE, RMSE/std, MAE, R2). Given what you now know about model performance on test data, can we still use this model to predict bandgap values of materials for making single-junction solar cell, which requires a bandgap between 1.1 and 1.7eV?  \n",
    "3. Again think about the use case for identifying high band gap materials above 3 eV, could the model differentiate between low and high bandgap materials?\n",
    "\n",
    "[Check or Submit Answers](#a5.2)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"exercise\">\n",
    "\n",
    "### Exercise 5.3<a name=\"e5.3\"></a>\n",
    "Questions:  \n",
    "1. Compare both the parity plots and performance statistics for the training and test set. Is the model performing better on one set than the other, or is there no difference? (No calculation needed.)  \n",
    "2. Which of the following most accurately describes this model: Underfit, overfit, or neither?\n",
    "3. Should we use training data or test data to estimate model prediction performance?\n",
    "\n",
    "\n",
    "[Check or Submit Answers](#a5.3)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"lesson\">\n",
    "\n",
    "### Visualizing and interpreting the decision tree model (OPTIONAL)<a name = \"lesson5.3\"></a>\n",
    "\n",
    "<div class = \"cellContent\">\n",
    "\n",
    "It's always important and helpful to understand how your ML model makes predictions - even more so for materials scientists, who are more likely to trust the model prediction and fabricate the recommended materials if the prediction has an explanation that matches their physical intuition (or defies it in a scientifically justifiable way).\n",
    "\n",
    "However, it's often a challenge because many ML models are \"**black boxes**\", the inner workings of which are not interpretable to humans at all. Decision trees are chosen for this lab partly because they are more interpretable than many other models. Even so, it becomes harder to extract physical instinct from them as they grow larger.\n",
    "\n",
    "As a final step, we will visualize the default decision tree model to help you understand what we've built. While the visualization doesn't quantify its performance, it illustrates which features are important and explains the difference between model performances on training and test data.\n",
    "\n",
    "</div> <!-- closes content -->\n",
    "\n",
    "</div> <!-- closes lesson -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"info\">\n",
    "\n",
    "### *Large cell output!*\n",
    "\n",
    "\n",
    "<div class = \"cellContent\">\n",
    "    \n",
    "The output of the next cell is very large! Click the left side of the output to expand it for a full view, or click again to minimize it.  \n",
    "\n",
    "Minimizing the output after viewing also makes it easier to interact with the rest of the notebook.\n",
    "\n",
    "</div> <!-- closes content -->\n",
    "</div> <!-- closes protip -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generate an image of the default decision tree\n",
    "dot_data = sklearn.tree.export_graphviz(Default_model.estimators_[0],out_file=None,feature_names=features_df_lowcorr.columns,filled=True,rounded=True,special_characters=True)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"info\">\n",
    "\n",
    "<div class = \"cellContent\">\n",
    "    \n",
    "Locate the decision node `Density_composition_average ≤ 0.059` in the visualization above using Ctrl+F (Cmd+F on Macs).\n",
    "\n",
    "</div> <!-- closes content -->\n",
    "</div> <!-- closes protip -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This visualization explicitly constructs each node in the default tree. Using the decision node `Density_composition_average ≤ 0.059` as an example, \n",
    "\n",
    " It lists which feature the node splits on, gives the mse for the data at that split, how many samples are at the node, and the value of the estimated band gap if it was a leaf node.  \n",
    "It should be immediately apparent that the tree is incredibly large, and and so we'll pick a few things to focus on as we look through it briefly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"exercise\">\n",
    "\n",
    "### Exercise 5.4<a name=\"e5.4\"></a>\n",
    "Questions:  \n",
    "1. Looking at the Density_composition_average node identified above, how many samples from the training data are included at this node?  \n",
    "2. Trace the branch down from this node until you find a leaf node. How many samples are included at this leaf node?  \n",
    "3. Inspect a few other leaf nodes until you identify how many samples are at the majority of leaf nodes, how many samples is this?  \n",
    "4. based on your previous analysis of overfitting, underfitting, or well fit does this support that previous claim about how the model is fit?  \n",
    "\n",
    "\n",
    "[Check or Submit Answers](#a5.4)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=blue> challenge (optional): We're currently leaving out a fairly small percentage of the data, 10%. Try going back and changing this to a few different values to see how it changes the results. For example try: 5%, 25%, 50%, 75%.  \n",
    "To do this you can edit the test_fraction parameter at the start of Section 4 and rerun the cells after that.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"section\">\n",
    "\n",
    "# 6. Improving the Model by Optimizing Hyperparameters<a name=\"6\"></a>\n",
    "---\n",
    "[Back to TOC](#toc)\n",
    "\n",
    "<div class = \"cellContent\">\n",
    "\n",
    "## Overview (edited)\n",
    "\n",
    "In the previous section, we evaluated the performance of a default model based on its performance in predicting a single set of test data. While this model predicted the training data almost perfectly, predictions of the testing data showed significantly worse performance. This combination of a model that perfectly reproduces training data, but can't predict data it hasn't seen is a classic sign of <u>overfitting</u>. One way to address this issue to to try to alter the hyperparameters of the model.\n",
    "\n",
    "When building our first model we used default settings from the `scikit-learn` package, we can attempt to improve the performance by optimizing the <u>hyperparameters</u> of a model (think of them as *tunable knobs*). \n",
    "\n",
    "This will be the penultimate task in this lab, before we select a final model and make predictions.\n",
    "\n",
    "Remember that a machine learning model has two types of parameters:  \n",
    " - The first are the regular <u>parameters</u> of the model. These would be the coefficients in the case of a linear model; in the case of the decision tree, they are the nodes that get generated when the model is fit, and we don't set them directly.  \n",
    " - The second are the <u>hyperparameters</u>. These are the variables that we adjust to change how the model learns and how it gets fit. For example, for decision trees, we can limit how complex and large the tree can be by setting a number of hyperparameters including the maximum tree depth and the maximum leaf nodes. We can also increase the number of trees that we train and make a model that is an average prediction of these multiple trees.  \n",
    "\n",
    "In this lab, we'll focus in on one hyperparameter specifically called <i>n\\_estimators</i> . Each estimator is one decision tree, and by using the RandomForestRegressor class from sci-kit-learn we can increase this number to build multiple individual trees which will all try to predict our bandgaps. Taking average results from multiple decision trees can often improve model performance and reduces overfitting that can occur in a single decision tree.\n",
    "\n",
    "\n",
    "\n",
    "## Learning Outcomes\n",
    "1. Identify a hyperparameter for the random forest model\n",
    "2. Perform a series of grid searches to find an optimal hyperparameter\n",
    "3. Evaluate model performance for different hyperparameter configurations\n",
    "4. Choose an optimal hyperparameter configuration\n",
    "5. Assess overfitting from learning curves\n",
    "6. Recall final optimized model training step before predictions can be made\n",
    "7. Compare model validation techniques: k-fold CV vs. train/test split\n",
    "8. Evaluate model performance for two different applications\n",
    "9. Recall decision tree (as evidence from inspections) is in high variance regime\n",
    "10. Recall random forest allows smoothing/averaging across many trees to reduce variance\n",
    "\n",
    "\n",
    "</div> <!-- closes content -->\n",
    "\n",
    "</div> <!-- closes section -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"lesson\">\n",
    "\n",
    "### Defining a parameter space<a name = \"lesson6.2\"></a>\n",
    "\n",
    "<div class = \"cellContent\">\n",
    "\n",
    "To optimize the hyperparameters we will perform a series of \"grid searches\". As the name implies we will define a grid of hyperparameter values to try, and then build models at each of those grid points. By assessing model performance at each grid point we can make a plot to see how model performance changes as a function of the hyperparameters. \n",
    "\n",
    "As mentioned above we'll be focusing on just varying one hyperparameter, but fundamentally we could vary any number of them at a time and make a higher dimensional grid that explores them all at once. The fundamental contraint to this type of strategy is the amount of time we are willing to wait for results. The more grid point we define, the more models we need to build, and therefore the more time it will take for the code to complete.\n",
    "\n",
    "</div> <!-- closes content -->\n",
    "\n",
    "</div> <!-- closes lesson -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Default model uses the following hyperparameters:\\n') # print default hyperparameters used\n",
    "pprint(Default_model.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up hyperparameter grid (a dictionary of hyperparameter candidates that we want the optimization strategy to consider)\n",
    "\n",
    "# EDIT LIST TO TRY DIFFERENT VALUES!\n",
    "### MAKE EDITS BELOW HERE ###\n",
    "\n",
    "number_of_trees = [1,10,25,50]\n",
    "\n",
    "### MAKE EDITS ABOVE HERE ###\n",
    "\n",
    "opt_dict = {'n_estimators':number_of_trees,'bootstrap':['True']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"lesson\">\n",
    "\n",
    "### Setting up a cross-validation scheme<a name = \"lesson6.1\"></a>\n",
    "\n",
    "<div class = \"cellContent\">\n",
    "\n",
    "Before we setup our optimization we'll want to readress our methodology for model assessment. Previously we made a single Train vs Test split in the data. This type of assessment is nice as a final check of a model once the hyperparameters are set, however think about what would happen if we only used that single Test split as our method for assessment. The we could find the best model at predicting that specific subset of data, but not the model that is best at predicting all of our available data. To adress this we'll use a cross-validation strategy from scikit-learn called `RepeatedKFold` cross-validation. The main different is that instead of making a single split in the data we will make 'K' splits in the data and predict each of those in turn by training on the remaining data. This means every data point is used in both training and testing. The \"Repeated\" addition to the KFold method means that the process is repeated a number of times so that there are a total of \"K\" x \"Repeats\" splits in the model. Now if we minimze the average error in predicting each split individually we can hopefully find a model that performns well in predicting all of our data!\n",
    "\n",
    "\n",
    "We'll set the number of splits to be 5, and also set a number of repeats to perform as 5. This means that in total we have 25 splits being generated in the training data.  \n",
    "\n",
    "Note: We're still leaving out the previously established testing data and not including it here. This is so we can go back after optimization and see how we do at predicting that data!\n",
    "\n",
    "</div> <!-- closes content -->\n",
    "\n",
    "</div> <!-- closes lesson -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kfold = KFold(n_splits=5,random_state=seed,shuffle=True)\n",
    "kfold = RepeatedKFold(n_splits=5,\n",
    "                      random_state=seed,\n",
    "                      n_repeats=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"lesson\">\n",
    "\n",
    "### Setting up a grid search<a name = \"lesson6.3\"></a>\n",
    "\n",
    "<div class = \"cellContent\">\n",
    "\n",
    "With the cross validation established, and a grid of hyperparameters assigned we just need to put everything together and build all combinations of models with the various splits and hyperparameters. Conveniently `scikit-learn` has build a method for doing just that.\n",
    "\n",
    "The `scikit-learn` documentation on hyperparameter tuning describes such a grid search as consisting of:\n",
    "\n",
    "1. an estimator (our decision tree regressor, for example);\n",
    "2. a parameter space;\n",
    "3. a method for searching or sampling candidates (we are limiting ourselves to a simple grid search in this lab);\n",
    "4. a cross-validation scheme; and\n",
    "5. a score function.\n",
    "\n",
    "*Note: While we are only demonstrating the simplest case of hyperparameter tuning in this lab, you should feel comfortable reading the scikit-learn documentation for continued learning after completing this lab: https://scikit-learn.org/stable/modules/grid_search.html*\n",
    "\n",
    "</div> <!-- closes content -->\n",
    "\n",
    "</div> <!-- closes lesson -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a grid search strategy\n",
    "\n",
    "import time\n",
    "\n",
    "CV = GridSearchCV(Default_model, # 1. the model whose hyperparamter is being optimized right now\n",
    "                  opt_dict,   # 2. a dictionary of values that we want the grid search to use\n",
    "                  cv=kfold,   # 4. k-fold cross-validation strategy is used to define training and validation splits (note this is separate from test splits) to be used for each grid point\n",
    "                  return_train_score=True,\n",
    "                  scoring=['neg_mean_squared_error','r2','neg_mean_absolute_error'], # 5. the performance metrics to be reported at each grid point specified in opt_dict\n",
    "                  refit='neg_mean_squared_error')\n",
    "\n",
    "# perform grid search\n",
    "tic = time.perf_counter() # start timer\n",
    "\n",
    "CV = CV.fit(X_train,y_train)\n",
    "\n",
    "toc = time.perf_counter() # stop timer\n",
    "\n",
    "# print results\n",
    "print(f\"Grid search completed in {toc - tic:0.3f} seconds.\")\n",
    "print(CV.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"info\">\n",
    "\n",
    "### *Why do some score functions have a \"neg_\" prefix?*\n",
    "\n",
    "<div class = \"cellContent\">\n",
    "\n",
    "Some of the score functions below have \"neg\" or negative in front. That's just done for computational reasons and doesn't change the actual metrics. We'll invert them when analyzing later. \n",
    "\n",
    "\n",
    "</div> <!-- closes content -->\n",
    "\n",
    "</div> <!-- closes protip -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And just like that we've performed the grid search! to visualize the results see the code blocks below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"lesson\">\n",
    "\n",
    "### Visualizing the learning curve<a name = \"lesson6.4\"></a>\n",
    "\n",
    "<div class = \"cellContent\">\n",
    "\n",
    "How do we find out the best value for the hyperparameter `n_estimators` from the grid search?  \n",
    "Let's check how the performance changed as a function of `n_estimators` by plotting the average Mean Squared Error (MSE) for both the training and validation splits for each hyperparameter choice (specified in `opt_dict`).\n",
    "\n",
    "</div> <!-- closes content -->\n",
    "\n",
    "</div> <!-- closes lesson -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot number of trees vs train and test MSE\n",
    "\n",
    "opt_dict_array = opt_dict[\"n_estimators\"]                     # array of grid points (x-axis)\n",
    "train_mse = CV.cv_results_[\"mean_train_neg_mean_squared_error\"] # MSE of training set at each grid point (y-axis)\n",
    "test_mse = CV.cv_results_[\"mean_test_neg_mean_squared_error\"]   # MSE of test set at each grid point (y-axis)\n",
    "\n",
    "fig1,ax1 = plt.subplots(figsize=(8,4))\n",
    "ax1.scatter(opt_dict_array, -train_mse)\n",
    "ax1.scatter(opt_dict_array, -test_mse)\n",
    "# ax1.fill_between(opt_dict_array, -train_mse, -test_mse, alpha=0.1)\n",
    "ax1.set_xlabel('Number of Trees')\n",
    "ax1.set_ylabel('Mean Squared Error')\n",
    "ax1.set_title('Compare training and validation MSE vs hyperparameter')\n",
    "plt.legend([\"Training\",\"Validation\",\"difference\"])\n",
    "plt.show()\n",
    "print(\"Minimum Mean Squared Error: \", round(min(-test_mse),4))\n",
    "print(\"Number of Trees at minimum: \", opt_dict_array[np.argmin(-test_mse)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"exercise\">\n",
    "\n",
    "### Exercise 6.1<a name=\"e6.1\"></a>\n",
    "Before diving into the questions lets run a few different grid searches to get a feel for how it works. By default the notebook is setup with a very rough grid search which should run quickly. be careful adding too many grid points because it is possible to slow down the grid search to the point of not finishing in hours or days. As a rule of thumb lets not set number of trees to be above 100, and lets not include more than 10 individual grid points in any one search. Try to get a sense of how performance varies with number of trees. Before answering questions below set your grid to be [1,3,5,7,10,15,20,50] which should give a reasonable spread of values. As a reminder these edits are made in the section above \"defining a parameter space\" \n",
    "\n",
    "Questions:  \n",
    "1. Looking just at the training data curve above, what is the number for `n_estimators` that gives the smallest MSE?\n",
    "2. How about for the validation data? Which value for `n_estimators` gives the smallest MSE?  \n",
    "3. Look at the trend in validation data. If n_estimators was increased beyond 50 do you think the model would change in performance significantly?  \n",
    "4. Do we expect the model get more complex as we increase the number of trees?  \n",
    "5. Looking at just the validation curve, does increasing number of trees seem to increase overfitting at any point on the curve? This would show up as worsening performance in the validation data as trees increase.\n",
    "\n",
    "[Check or Submit Answers](#a6.1)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"lesson\">\n",
    "\n",
    "### Default vs. optimized model: cross-validation performance<a name = \"lesson6.5\"></a>\n",
    "\n",
    "<div class = \"cellContent\">\n",
    "\n",
    "Looking at the previous learning cuve of performance versus the number of trees we can see the more trees seems to be giving better performance, however past 20 trees the rate of improvement seems to decrease dramatically. Going forward we'll assume we're using a forest of 50 trees which should give significant improvement in performance compared to where we started.\n",
    "\n",
    "Note: Recall that at each grid point, we are splitting the dataset into 5 random folds and repeating this process 5 times, which means we have trained 50 different decision tree models using the same hyperparameter at that grid point total. Every error metric you see below is the average value across 25 different models with the same hyperparameters but different splits of the training data.\n",
    "\n",
    "Error metrics below are for models with the best hyperparameter from the previous grid search, so make sure the last time you ran the grid search the largest number of trees was 50! You can check if you did this correctly with the cell block below, the `n_estimators` value should be 50\n",
    "\n",
    "</div> <!-- closes content -->\n",
    "\n",
    "</div> <!-- closes lesson -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check what the best parameters identified in the grid search were\n",
    "CV.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract cross validation performance metrics for the optimized model\n",
    "opt_CV_stats = CV_best_stats(CV,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare back to our default model we can construct another grid search that only uses \"1\" for `n_estimators`. That way it will still be the best model available.\n",
    "\n",
    "We do CV on a single grid point (\"1\"), build 25 different models, and average across them to get the results below.\n",
    "\n",
    "Now we can directly compare the model's performance on these metrics generated from the Kfold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_opt_dict = {'n_estimators':[1]}\n",
    "\n",
    "default_CV = GridSearchCV(Default_model,\n",
    "                          default_opt_dict,\n",
    "                          cv=kfold,\n",
    "                          return_train_score=True,\n",
    "                          scoring=['neg_mean_squared_error','r2','neg_mean_absolute_error'],\n",
    "                          refit='neg_mean_squared_error')\n",
    "default_CV = default_CV.fit(X_train,y_train)\n",
    "\n",
    "default_CV_stats = CV_best_stats(default_CV,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"exercise\">\n",
    "\n",
    "### Exercise 6.2<a name=\"e6.2\"></a>\n",
    "Questions:  \n",
    "1. Do we get improvement in the RMSE between the default and optimized model? What is the percentage improvement ($|RMSE_{test opt}-RMSE_{test default}|/RMSE_{test default}$)?   \n",
    "\n",
    "For this lab we'll assume that the RMSE metric gives a reasonable error estimate on predictions. So any prediction we made we'll assume it has + or - the CV RMSE value.   \n",
    "2. Assuming this level of accuraccy from the optimized model. Is our model accurate enough to predict single-junction solar materials? where the key design metric is having a band gap between 1.1 eV and 1.7 eV?  \n",
    "3. How about our other task. Is the optimized model accurate enough to predict high bandgap materials? where the key metric is ensuring predictions are above 3 eV?\n",
    "\n",
    "[Check or Submit Answers](#a6.2)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"lesson\">\n",
    "\n",
    "### Default vs. optimized model: training and test data performance<a name = \"lesson6.6\"></a>\n",
    "\n",
    "<div class = \"cellContent\">\n",
    "\n",
    "The goal is to optimize performance on unseen data\n",
    "\n",
    "Now, remember that we also have the set of Test data that we held out before any hyperparameter optimization. Lets see how the model has changed in predicting that data. We'll make the same two parity plots as before where we use the new optimized hyperparameters.\n",
    "\n",
    "We use the optimized hyperparameters to refit the model using the entire training set.\n",
    "\n",
    "</div> <!-- closes content -->\n",
    "\n",
    "</div> <!-- closes lesson -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refit the model using the best hyperparameters\n",
    "DT2 = CV.best_estimator_.fit(X_train,y_train)\n",
    "\n",
    "# predict both the train and test data\n",
    "Train_predictions2 = DT2.predict(X_train)\n",
    "Test_predictions2 = DT2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parity_plots_side_by_side(y_train,Train_predictions,y_train,Train_predictions2,title_left=\"Training Parity Plot (Default Model)\",title_right=\"Training Parity Plot (Optimized Model)\") # build both plots\n",
    "parity_stats_side_by_side(y_train,Train_predictions,y_train,Train_predictions2,\"Training Set (Default Model)\",\"Training Set (Optimized Model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"exercise\">\n",
    "\n",
    "### Exercise 6.3<a name=\"e6.3\"></a>\n",
    "Questions:  \n",
    "1. Look at both the parity plots and the training data statistics. Does the optimized model do better or worse at predicting the training data than the default model?  \n",
    "2. Do prediction performances on training data give you enough information to decide which model is more likely to give better predictions on Si and $\\text{SiO}_2$, which are not in the training set or the test set? Another way to ask this is does the training data result tell us anything about the predictive power of the model?\n",
    "\n",
    "[Check or Submit Answers](#a6.3)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parity_plots_side_by_side(y_test,Test_predictions,y_test,Test_predictions2, title_left=\"Test Parity Plot (Default Model)\",title_right=\"Test Parity Plot (Optimized Model)\") # build both plots\n",
    "parity_stats_side_by_side(y_test,Test_predictions,y_test,Test_predictions2,\"Test Set (Default Model)\",\"Test Set (Optimized Model)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"exercise\">\n",
    "\n",
    "### Exercise 6.4<a name=\"e6.4\"></a>\n",
    "Questions:  \n",
    "1. Just looking at the testing data statistics, does the optimized model do better or worse at predicting the testing data?\n",
    "2. Compare the difference between train and test RMSE for the default and optimized model. Did the difference between training and test performance increase or decrease after hyperparameter optimization?  \n",
    "3. Is this evidence that the optimized model is more overfit or less overfit?  \n",
    "[Check or Submit Answers](#a6.4)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"exercise\">\n",
    "\n",
    "### Exercise 6.5<a name=\"e6.5\"></a>\n",
    "Questions:  \n",
    "\n",
    "1. Based on your answers to exercise 6.3 and 6.4, if you were to choose between the default model and the optimized model, what decision will you make? Give at least 3 criteria for your evaluation.\n",
    "\n",
    "[Check or Submit Answers](#a6.5)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"section\">\n",
    "\n",
    "# 7. Making Predictions<a name=\"7\"></a>\n",
    "---\n",
    "[Back to TOC](#toc)\n",
    "\n",
    "<div class = \"cellContent\">\n",
    "\n",
    "## Overview\n",
    "In the final section of the lab we'll do one final test to gauge the model's performance. We've been trying throughout the lab to highlight the importance of keeping a model's use case in mind throughout the process and building and training. In our case we are focusing on two potential use cases. In predicting solar materials which require a bandgap between 1.1 and 1.7 eV, and wide bandgap materials which want a bandgap above 3 eV. \n",
    "\n",
    "In the more general sense, important questions to ask are things like:  \n",
    "1) what is the necessary accuracy for predictions I want to make? For example we've highlighted two use cases for our band gap model, one that requires a much more accurate model than the other.  \n",
    "2) What is the feature space I want to predict in (What compositions are we interested in predicting)? Our dataset is trained entirely on binary and elementary semiconductors. Making predictions outside of this space would be very risky as a lot of our error estimates might break down.\n",
    " \n",
    "\n",
    "## Learning Outcomes\n",
    "1. Judge model performance for two different applications\n",
    "2. Assess previous error predictions\n",
    "3. Propose improvements to an existing model\n",
    "\n",
    "\n",
    "</div> <!-- closes content -->\n",
    "\n",
    "</div> <!-- closes section -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember back when we first trained the model and predicted Silicon and Silica? Let do the same thing for fun with the optimized model. The values have likely shifted.\n",
    "\n",
    "When we fit the DT3 model we use the X_predict and y_predict versions of the dataset in which we removed 5 compounds so that we could predict them now. Note that these predictions are a bit artificial because when we did the model optimization this data was included. In a true research environment this isn't something you'd want to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model to all data except for the values we want to predict.\n",
    "DT3 = CV.best_estimator_.fit(X_predict,y_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit the cell below to change which compound is predicted between: Silicon, Silica, Salt, Diamond, and Tin\n",
    "\n",
    "Change the Prediction_features object to one of the following:  \n",
    "xpredict_Si  \n",
    "xpredict_SiO2  \n",
    "xpredict_NaCl  \n",
    "xpredict_C  \n",
    "xpredict_Sn  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAKE EDITS BELOW HERE ###\n",
    "\n",
    "Prediction_features = xpredict_Si\n",
    "\n",
    "### MAKE EDITS ABOVE HERE ###\n",
    "\n",
    "# make a prediction with the trained DT3 model\n",
    "print(\"Predicted Band Gap: \",DT3.predict(Prediction_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for our final test on model performance. We are going to take the individual predictions on our Test data set and quantify how often the model succeeded or failed in making predictions for both the Solar application and Wide Band Gap application. Below we've rearranged the existing data from the parity plots in the previous section and printed it explicitly so we can look in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In doing this we are viewing the results of this regression model through the lens of classificaiton. Essentially the materials with known values in a certain range will be viewed as one class of materials, and everything else as another class. We'll then assess how well the model does at correctly identifying these classes of materials. If you want you read up on the background related to a few of these metrics you can look into the metrics precision and recall for binary classifiers. During the exercises below we'll walk through the process of calculating the recall for this pseudo-classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine previous data into one dataframe for visualization\n",
    "predictions_combined = pd.DataFrame(list(zip(y_test,Test_predictions2)),columns=['test','predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sort on the Test values from low to high\n",
    "predictions_combined.sort_values(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"exercise\">\n",
    "\n",
    "### Exercise 7.1<a name=\"e7.1\"></a>\n",
    "Questions:  \n",
    "\n",
    "The first task was a use case where we want to predict single-junction solar materials and they need to have a band gap between 1.1 eV and 1.7 eV to have decent efficiency.  \n",
    "1. In the Test dataset how many materials do we have with band gaps within the range of being a good solar material? Note in terms of classification we are identifying the number of positive cases in the dataset.  \n",
    "2. Now we'll compare to the Predictions. Find the number of times the positive cases identified in question 1 are predicted to be single-junction solar materials (between 1.1 and 1.7 eV). This is referred to as the number of true positives, the number of times the positive cases were predicted correctly. Divide this number of true positives by the total number of positive cases (from question 1) to obtain the recall value. What is the recall of our pseudo-classifier to predict single-junction solar materials?\n",
    "\n",
    "The second task was to identify high bandgap materials such as GaN, where the bandgap at at or above 3 eV.  \n",
    "3. In the Test dataset how many materials do we have with band gaps at or above 3 eV?  \n",
    "4. Perform the same process from question 2 (remember are classes are now defined differently for this new task) and calculate the recall for predicting high bandgap materials. What is the recall in this case?\n",
    "\n",
    "Think about the performance of the model in correctly completing both of the prediction tasks on the test dataset.\n",
    "5. Based on the evidence from questions 1-4 which tasks can the model succeed at?\n",
    "\n",
    "Challenges (optional questions):\n",
    "6. There are a number of other classification metrics we could have used to assess the performance of the model on the two potential tasks. Calculate the precision and false discovery rate for the two tasks. Do these metrics support the conclusion you made in question 5? We ignored this earlier in favor of simplicity, but potentially there is a more nuanced understanding of how the models might perform.\n",
    "\n",
    "[Check or Submit Answers](#a7.1)\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The End"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, are we officially done? What's next in the machine learning workflow?\n",
    "\n",
    "- hopefully you have your research problem down at this point. or else, figure out what you need to know, and whether ML can help with that\n",
    "- go back and reiterate on hyperparameter tuning\n",
    "- use a different model\n",
    "    - get uncertainty estimate on your prediction by going from (decision) tree to (random) forest\n",
    "- redo data cleaning/featurization\n",
    "- get more data\n",
    "\n",
    "what should you do next as a student?\n",
    "\n",
    "if this is only interesting to you, and you don't plan to do ML yourself in the near future: solidify the big ideas and key takeaways.\n",
    "\n",
    "if you want to get hands-on with ML:\n",
    "- think about your data  \n",
    "- go through the lab again and figure out each line of code  \n",
    "- change parameters and do all the challenges  \n",
    "- read the docs for software packages such as scikit-learn or mastml that help us perform these machine learning workflows.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div class = \"answers\">\n",
    "\n",
    "### Answers<a name=\"answers\"></a>\n",
    "\n",
    "* [Back to TOC](#toc)\n",
    "\n",
    "\n",
    "These are the answers for the code exercises.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Back to Exercise 1.1](#e1.1) <a name=\"a1.1\"></a>\n",
    "1. There are duplicate entries for the same chemical formula  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Back to Exercise 1.2](#e1.2) <a name=\"a1.2\"></a>\n",
    "\n",
    "1. 1447 data points  \n",
    "2. 467 data points  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Back to Exercise 1.3](#e1.3) <a name=\"a1.3\"></a>\n",
    "1. 13.591 eV  (13.096 eV if using the corrected averaging)  \n",
    "2. 5 eV would be much too large to be a useful prediction, being more than twice the standard deviation of the data  \n",
    "3. 0.5 eV would be a useful prediction error, being less than a quarter of the standard deviation in the dataset  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Back to Exercise 1.4](#e1.4) <a name=\"a1.4\"></a>\n",
    "1. No, there is significantly more data between 0 and 1 eV  \n",
    "2. No, making predictions above 5 eV might be unreasonable because we have so little data at the high end.  \n",
    "3. We might consider throwing out some low bandgap data to make this more balanced.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Back to Exercise 1.5](#e1.5) <a name=\"a1.5\"></a>\n",
    "1. O, Se, S, Te, As  \n",
    "2. Lu, Tm, Ir, Y, Ta  \n",
    "3. Most confident in predictions of Oxides, then predictions containing Iridium, and least confident in predictions of an element that the model hasn't seen before "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Back to Exercise 2.1](#e2.1) <a name=\"a2.1\"></a>\n",
    "1. 87 (scroll down to the bottom of the dataframe to see the nubmer of columns - each column represents a feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Back to Exercise 2.2](#e2.2) <a name=\"a2.2\"></a>\n",
    "1. \\begin{equation} \\frac{3 * 3 + 51 * 1}{1 + 3} = 15 \\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Back to Exercise 3.1](#e3.1) <a name=\"a3.1\"></a>\n",
    "1. 86  \n",
    "2. No  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Back to Exercise 3.2](#e3.2) <a name=\"a3.2\"></a>\n",
    "1. 71  \n",
    "2. No (would be if very low like less than 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Back to Exercise 4.1](#e4.1) <a name=\"a4.1\"></a>\n",
    "1. No the train and test splits are not identical, there may be some differences between train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Back to Exercise 5.1](#e5.1) <a name=\"a5.1\"></a>\n",
    "1. \n",
    "    A. Yes, the predictions line up with the ideal line really well.  \n",
    "    B. No.  \n",
    "    C. No, there is no consistent deviation from the ideal line.  \n",
    "2. Yes.  \n",
    "3. Yes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Back to Exercise 5.2](#e5.2) <a name=\"a5.2\"></a>\n",
    "1.  \n",
    "    A. Yes. The majority of predictions still lines up with the ideal line.  \n",
    "    B. Yes, there are around 5-6 outliers spread out across the range of measured bandgaps.  \n",
    "    C. No. Although there is more overall deviation from the ideal line, the predictions are spread out evenly on both sides of the ideal line across the entire range and show no consistent over/underpredicting.  \n",
    "2. No. This use case requires that the prediction error doesn't exceed 0.3eV, but the test data RMSE is 1.2615 eV.  \n",
    "3. Yes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Back to Exercise 5.3](#e5.3) <a name=\"a5.3\"></a>\n",
    "1. Better on training set, worse on test set.  \n",
    "2. Overfit  \n",
    "3. we need to use test data because the goal of ML, in our case, is to predict properties of materials that the model hasn't seen before, and test set performance is more representative of the prediction task.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Back to Exercise 5.4](#e5.4) <a name=\"a5.4\"></a>\n",
    "1. 10  \n",
    "2. 1  \n",
    "3. 1  \n",
    "4. This supports the idea of the model being overfit as each training data point has it's own leaf node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Back to Exercise 6.1](#e6.1) <a name=\"a6.1\"></a>\n",
    "\n",
    "1. 50  \n",
    "2. 50  \n",
    "3. No, the model isn't changing as maximum leaf nodes increases beyond 20  \n",
    "4. Yes, the model is increasing in complexity as we add more trees  \n",
    "5. No, the model consistently improves in performance so it doesn't look like overfitting is increasing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Back to Exercise 6.2](#e6.2) <a name=\"a6.2\"></a>\n",
    "\n",
    "1. Yes we see an improvement of 23.52%   \n",
    "2. No.  \n",
    "3. Yes.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Back to Exercise 6.3](#e6.3) <a name=\"a6.3\"></a>\n",
    "\n",
    "1. The optimized model is worse at predicting training data\n",
    "2. No, test or validation predictions are necessary to estimate predictive performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Back to Exercise 6.4](#e6.4) <a name=\"a6.4\"></a>\n",
    "\n",
    "1. The optimized model does better at predicting test data.  \n",
    "2. The difference between training and test performance decreased after hyperparameter optimization.  \n",
    "3. Less overfit, between the training data is more similar to test data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Back to Exercise 6.5](#e6.5) <a name=\"a6.5\"></a>\n",
    "\n",
    "1. In this lab, we use 3 main criteria for model selection:  \n",
    "    1. Higher cross-validation score, which measures model prediction performance on unseen data  \n",
    "    2. Better performance on test data, which also measures model prediction merformance on unseen data  \n",
    "    3. Less overfit, which suggests the model is less likely to be adversely affected by spurious patterns in the training data  \n",
    "    \n",
    "   The optimized model has a higher CV score (see exercise 6.3), better performance on test data (exercise 6.4) and is less overfit (exercise 6.4) than the default model. We choose the optimized model because we expect it to have better final prediction performance based on the 3 criteria listed above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Back to Exercise 7.1](#e7.1) <a name=\"a7.1\"></a>\n",
    "\n",
    "1. 7 materials between 1.1 and 1.7 eV  \n",
    "2. 1/7 of the materials is predicted in the range, ~14% of the time  \n",
    "3. 9 materials at or above 3 eV  \n",
    "4. 100% of materials are correctly predicted above 3 eV  \n",
    "5. The second task of identifying wide bandgap materials the model succeeds at  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
